{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling MDPs in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes defining MDPs must inherit from abstract class ```MDP```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class MDP(metaclass=abc.ABCMeta):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def transition(self, state, action):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def reward(self, state, action):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation in 2D grid with deceleration zone at the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Navigation(MDP):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.state_size = self.ndim\n",
    "\n",
    "        # grid constants\n",
    "        self.__size = tf.constant(self.size, dtype=tf.float32)\n",
    "        self.__center = tf.constant(self.center, dtype=tf.float32)\n",
    "        self.__goal = tf.constant(self.goal, dtype=tf.float32)\n",
    "\n",
    "        # numerical constants\n",
    "        self.__0_00 = tf.constant(0.00, dtype=tf.float32)\n",
    "        self.__0_99 = tf.constant(0.99, dtype=tf.float32)\n",
    "        self.__1_00 = tf.constant(1.00, dtype=tf.float32)\n",
    "        self.__2_00 = tf.constant(2.00, dtype=tf.float32)\n",
    "        self.__decay = tf.constant(self.decay, dtype=tf.float32)\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        # distance to center of grid\n",
    "        d = tf.sqrt(tf.reduce_sum(tf.square(state - self.__center), 1, keep_dims=True))\n",
    "\n",
    "        # deceleration_factor\n",
    "        deceleration = self.__2_00 / (self.__1_00 + tf.exp(-self.__decay * d)) - self.__1_00\n",
    "\n",
    "        # next position\n",
    "        next_state = state + deceleration * action\n",
    "        next_state = tf.clip_by_value(next_state, self.__0_00, self.__size)\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reward(self, state, action):\n",
    "        return -tf.reduce_sum(tf.abs(state - self.__goal), 1, keep_dims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 10000)\n",
    "d = np.abs(x - 5.0)\n",
    "y = 2 / (1 + np.exp(-2.0 * d)) - 1.00\n",
    "print('min = {0:.6f}, max = {1:.6f}'.format(np.min(y), np.max(y)))\n",
    "\n",
    "plt.plot(x, y, 'k-')\n",
    "plt.axis([0.0, 10.0, 0.0, 1.1])\n",
    "plt.xlabel('x coordinate')\n",
    "plt.ylabel('factor')\n",
    "plt.title('Deceleration')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding an MDP as a Recurrent Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDP_RNNCell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, mdp, batch_size):\n",
    "        self.mdp = mdp\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return mdp.state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return mdp.state_size + 1\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        actions = inputs\n",
    "        next_state =  self.mdp.transition(state, actions)\n",
    "        reward = self.mdp.reward(state, actions)\n",
    "        return tf.concat([reward, next_state], 1), next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDP_RNN(object):\n",
    "    \n",
    "    def __init__(self, mdp, batch_size):\n",
    "        self.cell = MDP_RNNCell(mdp, batch_size)\n",
    "    \n",
    "    def unroll(self, actions, initial_state=None):\n",
    "        # set initial state \n",
    "        if initial_state is None:\n",
    "            initial_state = self.cell.zero_state(self.cell.batch_size, dtype=tf.float32)\n",
    "\n",
    "        # dynamic time unrolling\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(\n",
    "            self.cell,\n",
    "            actions,\n",
    "            initial_state=initial_state,\n",
    "            dtype=tf.float32)\n",
    "\n",
    "        # separate reward and state series\n",
    "        outputs = tf.unstack(outputs, axis=2)\n",
    "        max_time = int(actions.shape[1])\n",
    "        reward_series = tf.reshape(outputs[0], [-1, max_time, 1])\n",
    "        state_series = tf.stack(outputs[1:], axis=2)\n",
    "\n",
    "        return reward_series, state_series, final_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the action optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionOptimizer(object):\n",
    "    \n",
    "    def __init__(self, loss, learning_rate=0.001, limits=None, actions=None):\n",
    "        self.loss = loss\n",
    "\n",
    "        # optimization hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # action constraints\n",
    "        self.limits = None\n",
    "        if limits is not None:\n",
    "            self.limits = tf.assign(actions, tf.clip_by_value(actions, limits[0], limits[1]))\n",
    "\n",
    "        # backprop via RMSProp\n",
    "        self.train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    def run(self, sess, epoch=100, show_progress=True):\n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        losses = []\n",
    "        for epoch_idx in range(epoch):\n",
    "            # backprop and update weights\n",
    "            sess.run(self.train_step)\n",
    "\n",
    "            # maintain action constraints if any\n",
    "            if self.limits is not None:\n",
    "                sess.run(self.limits)\n",
    "\n",
    "            # store and show loss information\n",
    "            loss = sess.run([self.loss])\n",
    "            losses.append(loss)\n",
    "            if show_progress:\n",
    "                print('Epoch {0:5}: loss = {1}\\r'.format(epoch_idx, loss), end='')\n",
    "        \n",
    "        return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the MDP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'ndim': 2,\n",
    "    'size': (10.0, 10.0),\n",
    "    'initial': (1.0, 1.0),\n",
    "    'goal': (8.0, 8.0),\n",
    "    'center': (5.0, 5.0),\n",
    "    'decay': 2.0,\n",
    "    'limits': (-1.0, 1.0)\n",
    "}\n",
    "\n",
    "# MDP model\n",
    "mdp = Navigation(**params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unroll the RNN for a given horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000  # number of parallel planners\n",
    "max_time = 11      # horizon\n",
    "state_size = 2     # grid 2D\n",
    "\n",
    "# actions to optimize\n",
    "actions = tf.Variable(\n",
    "    tf.truncated_normal(shape=[batch_size, max_time, state_size], stddev=0.05),\n",
    "    name=\"actions\")\n",
    "\n",
    "# unrolled MDP model\n",
    "rnn = MDP_RNN(mdp, batch_size)\n",
    "rewards, states, final_state = rnn.unroll(actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the optimization loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss based on total reward\n",
    "total = tf.reduce_sum(rewards, 1)\n",
    "loss = tf.reduce_mean(tf.square(total))\n",
    "\n",
    "# index of best solution among all planners\n",
    "best_batch_idx = tf.argmax(total,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "limits = tf.constant(params['limits'], dtype=tf.float32)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# optimize, babe!\n",
    "with tf.Session() as sess:\n",
    "    epoch = 200\n",
    "    learning_rate = 0.01\n",
    "    losses = ActionOptimizer(loss, learning_rate, limits, actions).run(sess, epoch)\n",
    "\n",
    "    # getting results\n",
    "    best_batch_idx = sess.run(best_batch_idx)\n",
    "    final_state = np.squeeze(sess.run(final_state)[best_batch_idx])\n",
    "    cost = np.squeeze(sess.run(total)[best_batch_idx])\n",
    "\n",
    "    plan = np.squeeze(sess.run(actions)[best_batch_idx])\n",
    "    s_series = np.squeeze(sess.run(states)[best_batch_idx])\n",
    "    r_series = np.squeeze(sess.run(rewards)[best_batch_idx])\n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"Done in {0:.6f} sec\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporting results\n",
    "print(\"Final state =\", final_state)\n",
    "print(\"Cost =\", cost)\n",
    "print()\n",
    "\n",
    "print(\"Action, State, Reward\")\n",
    "for a, s, r in zip(plan, s_series, r_series):\n",
    "    print(\"[{0:-10.6f}, {1:-10.6f}]\".format(a[0], a[1]), end=', ')\n",
    "    print(\"[{0:-10.6f}, {1:-10.6f}]\".format(s[0], s[1]), end=', ')\n",
    "    print(\"{0:-10.6f}\".format(r))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# plotting losses\n",
    "plt.subplot(121)\n",
    "plt.plot(losses, 'b-')\n",
    "plt.title('Total Loss')\n",
    "plt.xlabel(\"# iterations\")\n",
    "plt.ylabel(\"total loss\")\n",
    "plt.grid()\n",
    "\n",
    "# plotting rewards\n",
    "plt.subplot(122)\n",
    "plt.plot(r_series, 'b-')\n",
    "plt.title(\"Reward over time\")\n",
    "plt.xlabel(\"time-step\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "xlim, ylim = params['size']\n",
    "xcenter, ycenter = params['center']\n",
    "start, end = params['initial'], params['goal']\n",
    "\n",
    "# plot configuration\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis([0.0, xlim, 0.0, ylim])\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.grid()\n",
    "plt.title(\"Navigation trajectory and deceleration zone\", fontweight=\"bold\", fontsize=16)\n",
    "plt.xlabel(\"x coordinate\")\n",
    "plt.ylabel(\"y coordinate\")\n",
    "\n",
    "# deceleration zone\n",
    "npoints = 1000\n",
    "X, Y = np.meshgrid(np.linspace(0.0, xlim, npoints), np.linspace(0.0, ylim, npoints))\n",
    "D = np.sqrt((X - xcenter) ** 2 + (Y - ycenter) ** 2)\n",
    "Lambda = 2 / (1 + np.exp(-params['decay'] * D)) - 1.00\n",
    "\n",
    "ticks = np.arange(0.0, 1.01, 0.10)\n",
    "cp = plt.contourf(X, Y, Lambda, ticks, cmap=plt.cm.bone)\n",
    "plt.colorbar(cp, ticks=ticks)\n",
    "cp = plt.contour(X, Y, Lambda, ticks, colors='black', linestyles='dashed')\n",
    "# plt.clabel(cp, inline=True, fontsize=10)\n",
    "\n",
    "# actions\n",
    "positions = np.concatenate([[start], s_series])\n",
    "plt.quiver(positions[:-1, 0], positions[:-1, 1], plan[:, 0], plan[:, 1],\n",
    "           angles='xy', scale_units='xy', scale=1, color='dodgerblue', width=0.005,\n",
    "           label='actions')\n",
    "\n",
    "# states\n",
    "plt.plot(positions[:, 0], positions[:, 1], 'o', color='darkblue', markersize=8, label='states')\n",
    "\n",
    "# start and end\n",
    "plt.plot([start[0]], [start[1]], marker='X', markersize=15, color='limegreen', label='initial')\n",
    "plt.plot([end[0]], [end[1]], marker='X', markersize=15, color='crimson', label='goal')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
