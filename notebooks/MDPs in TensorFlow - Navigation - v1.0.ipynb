{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this IPython notebook, we'll show how to solve **Continuous Space-Action MDPs** (CSA-MDPs) defined as **Recurrent Models** in TensorFlow.\n",
    "\n",
    "CSA-MDP is a mathematical model useful to represent discrete-time continuous dynamic systems. We won't bother formalizing the models, but we'll restrict ourselves to only define the necessary bits of information for you to understand the overall approach of **Planning through Backpropagation**, ok?\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "This work is completely inspired by the work of Prof. Sanner and his PhD Students [Wu Ga](https://github.com/wuga214/) and Buser Say:\n",
    "\n",
    "<blockquote>\n",
    "**Scalable Planning with Tensorflow for Hybrid Nonlinear Domains**. arXiv preprint arXiv:1704.07511 (2017).\n",
    "</blockquote>\n",
    "\n",
    "Please [check it out](https://arxiv.org/abs/1704.07511) before diving into this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a running example, we'll consider the problem of navigation in a limited size 2D-grid with a (non-linear) deceleration zone in the center of the grid.\n",
    "\n",
    "The Navigation 2D problem is defined as a CSA-MDP model where:\n",
    "\n",
    "- the state space $\\mathcal{S}= \\{(x, y) \\in \\mathbb{R}^2 : \\mathbf{l} \\leq (x, y) \\leq \\mathbf{u} \\}$ is the set of valid positions in a continuous grid space where $\\mathbf{u}$ and $\\mathbf{l}$ are its upper and lower bounds;\n",
    "- the action space $\\mathcal{A} = \\{(\\Delta_x, \\Delta_y) \\in [-1.0, 1.0] \\times [-1.0, 1.0] \\}$ represents continuous displacements in x and y directions;\n",
    "- transition function $\\mathcal{T}(\\mathbf{s}, \\mathbf{a})$ is such that:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{T}(\\mathbf{s}, \\mathbf{a}) &= min(\\mathbf{u}, max(\\mathbf{l}, \\mathbf{p})) \\\\\n",
    "p &= \\mathbf{s} + \\lambda \\mathbf{a} \\\\\n",
    "\\lambda &= \\frac{2}{1 + e^{-\\beta d}} -1 \\\\\n",
    "d &= \\parallel \\mathbf{s} - \\mathbf{z} \\parallel\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathbf{z}$ is the center of the deceleration zone and $\\beta$ is a decay factor;\n",
    "- the reward function is given by $\\mathcal{R}(\\mathbf{s}, \\mathbf{a}, \\mathbf{s}') = - \\parallel \\mathbf{s} - \\mathbf{g} \\parallel $ where $\\mathbf{g}$ is the goal position; \n",
    "- the initial state $\\mathbf{s}^\\star \\in \\mathcal{S}$ is a known fixed position.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"files/img/navigation-2D-nonlinear-grid.png\" width=\"400px\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"files/img/navigation-2D-nonlinear-deceleration.png\" width=\"400px\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The objective is to find a sequence of actions $\\pi = \\langle a_{1:H} \\rangle$ that maximizes the cummulative reward gained over a given horizon $H$ from initial state $\\mathbf{s}^\\star$:\n",
    "$$\n",
    "\\pi = \\underset{a_{1:H}}{argmax} \\sum_{i=1}^{H} \\mathcal{R}(\\mathbf{s}_t, \\mathbf{a}_{t+1}, \\mathbf{s}_{t+1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "First things first, let's import all the necessary libraries into our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import abc\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling MDPs in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes defining MDPs must inherit from abstract class ```TF_MDP```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_MDP(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    @abc.abstractproperty\n",
    "    def state_size(self):\n",
    "        return\n",
    "    \n",
    "    @abc.abstractproperty\n",
    "    def action_size(self):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def transition(self, state, action):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def reward(self, state, action):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation in 2D grid with deceleration zone at the center\n",
    "\n",
    "Let's extend the base class ```TF_MDP``` to define our Navigation 2D model in TensorFlow. Note that the transition and reward functions are a direct translation of the formulas laid out in the Introduction section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Navigation(TF_MDP):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        # grid constants\n",
    "        self.__size = tf.constant(self.size, dtype=tf.float32)\n",
    "        self.__center = tf.constant(self.center, dtype=tf.float32)\n",
    "        self.__goal = tf.constant(self.goal, dtype=tf.float32)\n",
    "\n",
    "        # numerical constants\n",
    "        self.__0_00 = tf.constant(0.00, dtype=tf.float32)\n",
    "        self.__1_00 = tf.constant(1.00, dtype=tf.float32)\n",
    "        self.__2_00 = tf.constant(2.00, dtype=tf.float32)\n",
    "        self.__decay = tf.constant(self.decay, dtype=tf.float32)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.ndim\n",
    "    \n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.ndim\n",
    "        \n",
    "    def transition(self, state, action):\n",
    "        # distance to center of grid\n",
    "        d = tf.sqrt(tf.reduce_sum(tf.square(state - self.__center), 1, keep_dims=True))\n",
    "\n",
    "        # deceleration_factor\n",
    "        deceleration = self.__2_00 / (self.__1_00 + tf.exp(-self.__decay * d)) - self.__1_00\n",
    "\n",
    "        # next position\n",
    "        next_state = state + deceleration * action\n",
    "        next_state = tf.clip_by_value(next_state, self.__0_00, self.__size)\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reward(self, state, action):\n",
    "        # norm L-1 (manhattan distance)\n",
    "        # return -tf.reduce_sum(tf.abs(state - self.__goal), 1, keep_dims=True)\n",
    "        \n",
    "        # norm L-2 (euclidean distance)\n",
    "        return -tf.sqrt(tf.reduce_sum(tf.square(state - self.__goal), 1, keep_dims=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding an MDP as a Recurrent Neural Net\n",
    "\n",
    "Once the MDP transition and reward functions are in place in the computational graph, the next step towards representing the problem within a recurrent model is to encapsulate the MDP in a ```RNNCell``` as illustrated below.\n",
    "\n",
    "<img src=\"files/img/mdp-cell.png\" width=\"300px\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP_RNNCell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return mdp.state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return mdp.state_size + 1\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        # x_t: cell inputs\n",
    "        actions = inputs\n",
    "\n",
    "        # h_t: cell state\n",
    "        next_state =  self.mdp.transition(state, actions)\n",
    "\n",
    "        # yhat_y: cell output\n",
    "        reward = self.mdp.reward(next_state, actions)\n",
    "\n",
    "        # outputs\n",
    "        outputs = tf.concat([reward, next_state], 1)\n",
    "\n",
    "        return outputs, next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the recurrent model over a given horizon, we need to perform the time unrolling with respect to the actions variables that inherently defines the horizon (i.e., ```max_time```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP_RNN(object):\n",
    "    \n",
    "    def __init__(self, mdp):\n",
    "        self.cell = MDP_RNNCell(mdp)\n",
    "    \n",
    "    def unroll(self, actions, initial_state):\n",
    "        # time unrolling\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(\n",
    "            self.cell,\n",
    "            actions,\n",
    "            initial_state=initial_state,\n",
    "            dtype=tf.float32)\n",
    "\n",
    "        # separate reward and state series\n",
    "        outputs = tf.unstack(outputs, axis=2)\n",
    "        max_time = int(actions.shape[1])\n",
    "        reward_series = tf.reshape(outputs[0], [-1, max_time, 1])\n",
    "        state_series = tf.stack(outputs[1:], axis=2)\n",
    "\n",
    "        return reward_series, state_series, final_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the action optimizer\n",
    "\n",
    "At this point we have defined the recurrent model representing the MDP unrolled for a given horizon. Before being able to finding the plan via backpropagation we need to define optimization operations that will go on top of our computation graph.\n",
    "\n",
    "For this particular tutorial, we'll use the ```RMSPropOptimizer```. Note that the ```ActionOptimizer``` is parameterized by a ```loss``` function. We'll provide that in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionOptimizer(object):\n",
    "    \n",
    "    def __init__(self, loss, learning_rate=0.001, limits=None, actions=None):\n",
    "        self.loss = loss\n",
    "\n",
    "        # optimization hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # action constraints\n",
    "        self.limits = None\n",
    "        if limits is not None:\n",
    "            self.limits = tf.assign(actions, tf.clip_by_value(actions, limits[0], limits[1]))\n",
    "\n",
    "        # backprop via RMSProp\n",
    "        self.train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    def run(self, sess, epoch=100, show_progress=True):\n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        losses = []\n",
    "        for epoch_idx in range(epoch):\n",
    "            # backprop and update weights\n",
    "            sess.run(self.train_step)\n",
    "\n",
    "            # maintain action constraints if any\n",
    "            if self.limits is not None:\n",
    "                sess.run(self.limits)\n",
    "\n",
    "            # store and show loss information\n",
    "            loss = sess.run(self.loss)\n",
    "            losses.append(loss)\n",
    "            if show_progress:\n",
    "                print('Epoch {0:5}: loss = {1}\\r'.format(epoch_idx, loss), end='')\n",
    "        \n",
    "        return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together\n",
    "\n",
    "We're almost there. The final step before running our action optimizer to find a plan is to define a ```loss``` (i.e., cost function). For this, we must first instantiate a concrete MDP model and unroll it for a predefined horizon.\n",
    "\n",
    "<img src=\"files/img/mdp-recurrent.png\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the MDP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'ndim': 2,\n",
    "    'size': (10.0, 10.0),\n",
    "    'initial': (1.0, 1.0),\n",
    "    'goal': (8.0, 8.0),\n",
    "    'center': (5.0, 5.0),\n",
    "    'decay': 1.5,\n",
    "    'limits': (-1.0, 1.0)\n",
    "}\n",
    "\n",
    "# MDP model\n",
    "mdp = Navigation(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unroll the RNN for a given horizon\n",
    "\n",
    "An indispensable step in building the unrolled recurrent model is to create the parameters to be optimized in our recurrent model!\n",
    "\n",
    "In TensorFlow these are defined by the built-in tensors known as ```Variable```s. These are special tensors in the sense that they are the only ones that are persistent across executions in a ```Session```, which is absolutely necessary to perform batch optimization.\n",
    "\n",
    "Another important point to understand (actually this is the crux of the approach) is that differently from the usual Recurrent Neural Nets (RNNs), we are not trying to optimize the inner parameters of the cell's state and output functions. Instead:\n",
    "\n",
    "<blockquote>\n",
    "\"*we reverse the idea of training parameters of the network given fixed inputs to instead optimizing the inputs (i.e., actions) subject to fixed parameters (effectively the transition and reward parameterization assumed a priori known in planning)*\".\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000  # number of parallel planners\n",
    "max_time = 10       # horizon H\n",
    "state_size = 2     # grid 2D\n",
    "\n",
    "# actions to optimize\n",
    "actions = tf.Variable(\n",
    "    tf.truncated_normal(shape=[batch_size, max_time, state_size], stddev=0.05),\n",
    "    name=\"actions\")\n",
    "\n",
    "# initial state\n",
    "x_initial, y_initial = params['initial']\n",
    "x_initial = tf.fill([batch_size], tf.constant(x_initial, tf.float32))\n",
    "y_initial = tf.fill([batch_size], tf.constant(y_initial, tf.float32))\n",
    "initial_state = tf.stack([x_initial, y_initial], axis=1)\n",
    "\n",
    "# unrolled MDP model\n",
    "mdp_rnn = MDP_RNN(mdp)\n",
    "rewards, states, final_state = mdp_rnn.unroll(actions, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the optimization loss function\n",
    "\n",
    "Finally, we can define the graph operations necessary to optimize our recurrent model. \n",
    "\n",
    "Note how we use the batch optimization for defining **paralell planners** so as to mitigate local minima problems of the overall cost function - *remember nothing guarantees that the MDP's transition and reward functions will imply in a well-behaved loss function to be optimized!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative reward over horizon\n",
    "total = -tf.reduce_sum(rewards, 1)\n",
    "\n",
    "# Mean-Squared Error (MSE)\n",
    "loss = tf.reduce_mean(tf.square(total))\n",
    "\n",
    "# index of best solution among all planners\n",
    "best_batch = tf.argmax(total,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize it!\n",
    "\n",
    "Game is ON! Everything is in place for running the optimizer and finding a good plan for our Navigation 2D problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action domain constraint [-1.0, 1.0]\n",
    "limits = tf.constant(params['limits'], dtype=tf.float32)\n",
    "\n",
    "# optimization hyper-parameters\n",
    "epoch = 300\n",
    "learning_rate = 0.005\n",
    "\n",
    "# the start of the show!\n",
    "optimizer = ActionOptimizer(loss, learning_rate, limits, actions)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# optimize, babe!\n",
    "with tf.Session() as sess:\n",
    "    # run optimizer for a number of epochs\n",
    "    losses = optimizer.run(sess, epoch)\n",
    "\n",
    "    # get results per batch\n",
    "    total_cost_per_batch = sess.run(total)\n",
    "\n",
    "    # get results for best batch\n",
    "    best_batch_idx         = sess.run(best_batch)\n",
    "    best_batch_final_state = np.squeeze(sess.run(final_state)[best_batch_idx])\n",
    "    best_batch_total_cost  = np.squeeze(sess.run(total)[best_batch_idx])\n",
    "    best_batch_actions     = np.squeeze(sess.run(actions)[best_batch_idx])\n",
    "    best_batch_states      = np.squeeze(sess.run(states)[best_batch_idx])\n",
    "    best_batch_rewards     = np.squeeze(sess.run(rewards)[best_batch_idx])\n",
    "    end_points = sess.run(final_state)\n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"Done in {0:.8f} sec\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing results\n",
    "\n",
    "Hopefully the optimization went smoothly. Now, let's visualize the results. Just run the following notebook cells to get some nice charts and information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporting results\n",
    "print(\"Final state =\", best_batch_final_state)\n",
    "print(\"Cost =\", best_batch_total_cost)\n",
    "print()\n",
    "\n",
    "print(\"Action, State, Reward\")\n",
    "for a, s, r in zip(best_batch_actions, best_batch_states, best_batch_rewards):\n",
    "    print(\"[{0:-10.6f}, {1:-10.6f}]\".format(a[0], a[1]), end=', ')\n",
    "    print(\"[{0:-10.6f}, {1:-10.6f}]\".format(s[0], s[1]), end=', ')\n",
    "    print(\"{0:-10.6f}\".format(r))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# plotting losses\n",
    "plt.subplot(131)\n",
    "plt.plot(losses, 'b-')\n",
    "plt.title('Total Loss')\n",
    "plt.xlabel(\"# iterations\")\n",
    "plt.ylabel(\"total loss\")\n",
    "plt.grid()\n",
    "\n",
    "# plotting rewards\n",
    "plt.subplot(132)\n",
    "plt.plot(best_batch_rewards, 'b-')\n",
    "plt.title(\"Reward over time\")\n",
    "plt.xlabel(\"time-step\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.grid()\n",
    "\n",
    "# histogram of cumulative cost per batch\n",
    "plt.subplot(133)\n",
    "plt.hist(total_cost_per_batch, normed=True, histtype='stepfilled')\n",
    "plt.title('Distribution of cumulative cost per batch')\n",
    "plt.xlabel('total')\n",
    "plt.ylabel('density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_navigation(grid, states=None, actions=None, deceleration=True):\n",
    "\n",
    "    # params\n",
    "    xlim, ylim = grid['size']\n",
    "    xcenter, ycenter = grid['center']\n",
    "    start, end = grid['initial'], grid['goal']\n",
    "\n",
    "    # plot configuration\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis([0.0, xlim, 0.0, ylim])\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.grid()\n",
    "    plt.title(\"Navigation 2D\", fontweight=\"bold\", fontsize=16)\n",
    "    plt.xlabel(\"x coordinate\")\n",
    "    plt.ylabel(\"y coordinate\")\n",
    "\n",
    "    # deceleration zone\n",
    "    if deceleration:\n",
    "        npoints = 1000\n",
    "        X, Y = np.meshgrid(np.linspace(0.0, xlim, npoints), np.linspace(0.0, ylim, npoints))\n",
    "        D = np.sqrt((X - xcenter) ** 2 + (Y - ycenter) ** 2)\n",
    "        Lambda = 2 / (1 + np.exp(-grid['decay'] * D)) - 1.00\n",
    "        ticks = np.arange(0.0, 1.01, 0.10)\n",
    "        cp = plt.contourf(X, Y, Lambda, ticks, cmap=plt.cm.bone)\n",
    "        plt.colorbar(cp, ticks=ticks)\n",
    "        cp = plt.contour(X, Y, Lambda, ticks, colors='black', linestyles='dashed')\n",
    "        \n",
    "    # actions\n",
    "    if actions is not None:\n",
    "        positions = np.concatenate([[start], states])\n",
    "        plt.quiver(positions[:-1, 0], positions[:-1, 1], actions[:, 0], actions[:, 1],\n",
    "                   angles='xy', scale_units='xy', scale=1, color='dodgerblue', width=0.005,\n",
    "                   label='actions')\n",
    "    # states\n",
    "    if states is not None:\n",
    "        plt.plot(positions[:, 0], positions[:, 1], 'o', color='darkblue', markersize=8, label='states')\n",
    "\n",
    "    # start and end\n",
    "    plt.plot([start[0]], [start[1]], marker='X', markersize=15, color='limegreen', label='initial')\n",
    "    plt.plot([end[0]], [end[1]], marker='X', markersize=15, color='crimson', label='goal')\n",
    "    plt.annotate('({0}, {1})'.format(start[0], start[1]), xy=(start[0]-0.5, start[1]-0.5))\n",
    "    plt.annotate('({0}, {1})'.format(end[0], end[1]), xy=(end[0]-0.5, end[1]+0.5))\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_navigation(params, best_batch_states, best_batch_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a lot of planners (batch_size = 1000). We can see how many of them are close to the goal after the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold = 0.01\n",
    "x_inter = (params[\"goal\"][0] - 0.1, params[\"goal\"][0] + 0.5)\n",
    "y_inter = (params[\"goal\"][1] - 0.1, params[\"goal\"][1] + 0.5)\n",
    "\n",
    "x_goal = np.full(end_points.shape[0], params[\"goal\"][0])\n",
    "y_goal = np.full(end_points.shape[0], params[\"goal\"][1])\n",
    "all_targets = np.column_stack((x_goal, y_goal))\n",
    "\n",
    "# calculating the distance for all planners in the batch\n",
    "# using the euclidean distance\n",
    "result = end_points - all_targets\n",
    "result = result ** 2\n",
    "result = np.sum(result, axis=1)\n",
    "result = np.sqrt(result)\n",
    "good = result <= distance_threshold\n",
    "\n",
    "\n",
    "# using both pandas and numpy to plot the different points\n",
    "distance = []\n",
    "for value in good:\n",
    "    if value:\n",
    "        distance.append(\"Close\")\n",
    "    else:\n",
    "        distance.append(\"Distant\")\n",
    "        \n",
    "df = pd.DataFrame(data=end_points, columns=[\"x\", \"y\"])\n",
    "distance_collum = 'Distance to {}'.format(params[\"goal\"])\n",
    "df[distance_collum] = distance\n",
    "my_palette = sns.color_palette(\"muted\")\n",
    "sns.lmplot(x=\"x\", y=\"y\", hue=distance_collum, data=df, fit_reg=False, palette=my_palette)\n",
    "plt.title('Final state for all the planners in the batch')\n",
    "plt.show()\n",
    "\n",
    "print('Planners close to the goal: {}'.format(np.count_nonzero(good)))\n",
    "print('Planners not so close to the goal: {}'.format(good.shape[0] - np.count_nonzero(good)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End\n",
    "\n",
    "This is the end! I hope you've got something interesting out of this tutorial!\n",
    "\n",
    "Nevertheless, if you read the original paper and run this tutorial till the very end, but still have questions or suggestions, please feel free to drop a note on  ```thiago.pbueno``` @ ```gmail``` ```.com```.\n",
    "\n",
    "Thanks for getting so far. Now go do something useful with your life, bro...\n",
    "\n",
    "*Happy coding!!!*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
