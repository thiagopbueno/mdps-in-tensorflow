{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDPs in TensorFlow - Navigation with Noisy Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this IPython notebook, we'll explore **Continuous State-Action MDPs** with stochastic transitions in TensorFlow. All stochastic transitions will be defined by a deterministic function combined with external noise that is considered an input to the MDP cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import abc\n",
    "import functools\n",
    "import time\n",
    "\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling MDPs in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes defining MDPs must inherit from abstract class ```MDP```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    @abc.abstractproperty\n",
    "    def action_size(self):\n",
    "        return\n",
    "    \n",
    "    @abc.abstractproperty\n",
    "    def state_size(self):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def transition(self, state, action, noise=None):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def reward(self, state, action):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation in 2D grid with deceleration zone at the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Navigation(MDP):\n",
    "\n",
    "    def __init__(self, graph, grid, deceleration, max_theta=20):\n",
    "        self.graph = graph\n",
    "\n",
    "        self.ndim = grid[\"ndim\"]\n",
    "        self.max_theta = max_theta\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # grid constants\n",
    "            self.__size = tf.constant(grid[\"size\"], dtype=tf.float32)\n",
    "            self.__goal = tf.constant(grid[\"goal\"], dtype=tf.float32)\n",
    "\n",
    "            # deceleration constants\n",
    "            self.__center = tf.constant(deceleration[\"center\"], dtype=tf.float32)\n",
    "            self.__decay  = tf.constant(deceleration[\"decay\"],  dtype=tf.float32)\n",
    "\n",
    "            # numerical constants\n",
    "            self.__0_00 = tf.constant(0.00, dtype=tf.float32)\n",
    "            self.__1_00 = tf.constant(1.00, dtype=tf.float32)\n",
    "            self.__2_00 = tf.constant(2.00, dtype=tf.float32)\n",
    "            self.__8_00 = tf.constant(8.00, dtype=tf.float32)\n",
    "\n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.ndim\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.ndim\n",
    "        \n",
    "    def transition(self, state, action, noise):\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # apply rotation noise\n",
    "            cos = tf.cos(self.max_theta * np.pi / 180 * noise)\n",
    "            sin = tf.sin(self.max_theta * np.pi / 180 * noise)\n",
    "\n",
    "            noise_matrix = tf.stack([cos, -sin, sin, cos], axis=1)\n",
    "            noise_matrix = tf.reshape(noise_matrix, [-1, 2, 2])\n",
    "            noisy_action = tf.matmul(noise_matrix, tf.reshape(action, [-1, 2, 1]))\n",
    "            noisy_action = tf.reshape(noisy_action, [-1, 2])\n",
    "\n",
    "            # distance to center of deceleration zone\n",
    "            d = tf.sqrt(tf.reduce_sum(tf.square(state - self.__center), 1, keep_dims=True))\n",
    "\n",
    "            # deceleration_factor\n",
    "            deceleration = self.__2_00 / (self.__1_00 + tf.exp(-self.__decay * d)) - self.__1_00\n",
    "\n",
    "            # next position\n",
    "            next_state = state + deceleration * noisy_action\n",
    "            \n",
    "            # avoid getting out of map\n",
    "            next_state = tf.clip_by_value(next_state, self.__0_00, self.__size)\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reward(self, state, action):\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            # norm L-1 (manhattan distance)\n",
    "            # r = -tf.reduce_sum(tf.abs(state - self.__goal), 1, keep_dims=True)\n",
    "\n",
    "            # norm L-2 (euclidean distance)\n",
    "            r = -tf.sqrt(tf.reduce_sum(tf.square(state - self.__goal), 1, keep_dims=True))\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding an MDP as a Recurrent Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encapsulate MDP components into RNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP_RNNCell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, mdp, policy):\n",
    "        self.mdp = mdp\n",
    "        self.policy = policy\n",
    "\n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.mdp.action_size\n",
    "        \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.mdp.state_size + 1\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.mdp.state_size + self.mdp.action_size + 1\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "\n",
    "        with self.mdp.graph.as_default():\n",
    "\n",
    "            # add policy network\n",
    "            mdp_action = self.policy(state)\n",
    "\n",
    "            # separate MDP state and timestep\n",
    "            h = tf.unstack(state, axis=1)\n",
    "            sz = self.mdp.state_size\n",
    "            mdp_state = tf.stack(h[:sz], axis=1)\n",
    "            timestep  = tf.reshape(h[sz], [-1, 1])\n",
    "\n",
    "            # add MDP components to the RNN cell output\n",
    "            noise = inputs\n",
    "            mdp_next_state = self.mdp.transition(mdp_state, mdp_action, noise)\n",
    "            mdp_reward = self.mdp.reward(mdp_next_state, mdp_action)\n",
    "\n",
    "            # gather MDP state and timestep\n",
    "            cell_next_state = tf.concat([mdp_next_state, timestep + 1], axis=1)\n",
    "            \n",
    "            # concatenate outputs\n",
    "            outputs = tf.concat([mdp_reward, mdp_next_state, mdp_action], axis=1)\n",
    "\n",
    "        return outputs, cell_next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the MDP's policy as a Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(object):\n",
    "    \n",
    "    def __init__(self, graph, layers, limits=1.0):\n",
    "        self.graph = graph\n",
    "        self.policy = functools.partial(self.__build_network, layers, limits)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        return self.policy(state)\n",
    "    \n",
    "    def __build_network(self, layers, limits, state):\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            with tf.variable_scope('policy'):\n",
    "\n",
    "                # hidden layers\n",
    "                outputs = state\n",
    "                for i, n_h in enumerate(layers[1:]):\n",
    "                    if i != len(layers)-2:\n",
    "                        activation = tf.nn.relu\n",
    "                    else:\n",
    "                        activation = tf.nn.tanh\n",
    "\n",
    "                    outputs = tf.layers.dense(outputs,\n",
    "                                              units=n_h,\n",
    "                                              activation=activation,\n",
    "                                              kernel_initializer=tf.glorot_normal_initializer(),\n",
    "                                              name=\"layer\"+str(i+1))\n",
    "\n",
    "                # add action limits over last tanh layer\n",
    "                action = tf.constant(limits) * outputs\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unroll the model given a finite horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP_RNN(object):\n",
    "    \n",
    "    def __init__(self, mdp, policy, batch_size=1):\n",
    "        self.cell = MDP_RNNCell(mdp, policy)\n",
    "        self.graph = mdp.graph\n",
    "    \n",
    "    def unroll(self, max_time):\n",
    "\n",
    "        cell_state_size = self.cell.state_size\n",
    "        mdp_state_size  = self.cell.mdp.state_size\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # inputs (noise)\n",
    "            inputs = tf.placeholder(tf.float32, shape=(None, max_time, 1), name=\"inputs\")\n",
    "\n",
    "            # initial state\n",
    "            initial_state = tf.placeholder(tf.float32, shape=(None, cell_state_size), name=\"initial_state\")\n",
    "\n",
    "            # dynamic time unrolling\n",
    "            outputs, final_state = tf.nn.dynamic_rnn(\n",
    "                self.cell,\n",
    "                inputs,\n",
    "                initial_state=initial_state,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            # gather reward, state and action series\n",
    "            outputs = tf.unstack(outputs, axis=2)\n",
    "            reward_series = tf.reshape(outputs[0], [-1, max_time, 1])\n",
    "            state_series  = tf.stack(outputs[1:1+mdp_state_size], axis=2)\n",
    "            action_series = tf.stack(outputs[1+mdp_state_size:],  axis=2)\n",
    "\n",
    "        return reward_series, state_series, action_series, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Policy Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyOptimizer(object):\n",
    "    \n",
    "    def __init__(self, graph, loss, total, learning_rate, initial_state, noise_generator):\n",
    "        self.graph = graph\n",
    "\n",
    "        self.loss = loss\n",
    "        self.total = total\n",
    "        \n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "        self.noise = noise_generator\n",
    "        \n",
    "        # optimization hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            # backprop via RMSProp\n",
    "            self.train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "            # global initializer\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def run(self, sess, epoch=100, show_progress=True):\n",
    "        \n",
    "        # initialize variables\n",
    "        sess.run(self.init_op)\n",
    "        \n",
    "        losses = []\n",
    "        totals = []\n",
    "        for epoch_idx in range(epoch):\n",
    "            # generate inputs (noise)\n",
    "            inputs_data = self.noise()\n",
    "\n",
    "            # backprop and update weights\n",
    "            _, loss, total = sess.run([self.train_step, self.loss, self.total],\n",
    "                                      feed_dict={'initial_state:0': self.initial_state,\n",
    "                                                 'inputs:0': inputs_data})\n",
    "\n",
    "            # store total reward\n",
    "            total = np.mean(total)\n",
    "            totals.append(total)\n",
    "\n",
    "            # store loss information\n",
    "            losses.append(loss)\n",
    "\n",
    "            # show information\n",
    "            if show_progress:\n",
    "                print('Epoch {0:5}: loss = {1}\\r'.format(epoch_idx, loss, total), end='')\n",
    "        print()\n",
    "\n",
    "        return losses, totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the noise generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseGenerator(object):\n",
    "    \n",
    "    def __init__(self, batch_size, max_time):\n",
    "        self.size = (batch_size, max_time, 1)\n",
    "\n",
    "    def __call__(self):\n",
    "        noise = np.random.normal(size=self.size).astype(np.float32)\n",
    "        return noise\n",
    "    \n",
    "class PartialNoiseGeneratorSameTrajectory(object):\n",
    "\n",
    "    def __init__(self, batch_size, max_time):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_time = max_time\n",
    "\n",
    "    def __call__(self):\n",
    "        noise = np.random.normal(size=(1, self.max_time)).astype(np.float32)\n",
    "        noise = np.repeat(noise, [self.batch_size], axis=0)\n",
    "        noise = np.tril(noise)\n",
    "        noise = np.reshape(noise, noise.shape + (1,))\n",
    "        return noise\n",
    "    \n",
    "class PartialNoiseGeneratorDifferentTrajectory(object):\n",
    "\n",
    "    def __init__(self, batch_size, max_time):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_time = max_time\n",
    "\n",
    "    def __call__(self):\n",
    "        noise = np.random.normal(size=(self.batch_size, self.max_time)).astype(np.float32)\n",
    "        noise = np.tril(noise)\n",
    "        noise = np.reshape(noise, noise.shape + (1,))\n",
    "        return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_loss_function(graph, rewards):\n",
    "    \n",
    "    with graph.as_default():\n",
    "        total = tf.reduce_sum(rewards, axis=1)\n",
    "        loss  = tf.reduce_mean(tf.square(total))\n",
    "    \n",
    "    return total, loss\n",
    "\n",
    "def MSE_lambda_loss_function(graph, rewards):\n",
    "    \n",
    "    # trace-decay parameter lambda for controlling the bootstrap\n",
    "    lambda_discount_factor = 0.9 \n",
    "    lambda_factor = np.full(batch_size,\n",
    "                            (1 - lambda_discount_factor) / (1 - lambda_discount_factor ** batch_size),\n",
    "                            dtype=np.float32)\n",
    "    l = 1\n",
    "    for i in range(len(lambda_factor)):\n",
    "        lambda_factor[i] *= l\n",
    "        l *= lambda_discount_factor\n",
    "    lambda_factor = np.reshape(lambda_factor, (batch_size, 1))\n",
    "    \n",
    "    with graph.as_default():\n",
    "        total = tf.reduce_sum(rewards, axis=1)\n",
    "        average_total = total * tf.constant(lambda_factor)\n",
    "        loss  = tf.reduce_sum(tf.square(average_total))\n",
    "    \n",
    "    return total, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(graph, optimizer, epoch):\n",
    "\n",
    "    # saver\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # optimize it, babe!\n",
    "        losses, totals = optimizer.run(sess, epoch)\n",
    "\n",
    "        end = time.time()\n",
    "        uptime = end - start\n",
    "        print(\"Done in {0:.6f} sec.\\n\".format(uptime))\n",
    "\n",
    "        # save model\n",
    "        save_path = saver.save(sess, 'checkpoints/model.ckpt')\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    return losses, totals, saver, uptime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(graph, series, s0, batch_size, max_time):\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # restore learned policy model\n",
    "        saver.restore(sess, 'checkpoints/model.ckpt')\n",
    "\n",
    "        # sample noise data\n",
    "        noise = np.random.normal(size=(batch_size, max_time, 1)).astype(np.float32)\n",
    "\n",
    "        # simulate MDP trajectories\n",
    "        result = sess.run(series, feed_dict={'inputs:0': noise, 'initial_state:0': s0})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the initial state for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_initial_state(x0, y0, batch_size):\n",
    "    x_init = np.full([batch_size], x0, np.float32)\n",
    "    y_init = np.full([batch_size], y0, np.float32)\n",
    "    t_init = np.zeros([batch_size], np.float32)\n",
    "    return np.stack([x_init, y_init, t_init], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together in a testbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train our model! Let's first of all create the computational graph to which all necessary operations will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain parameters\n",
    "grid = {\n",
    "    'ndim': 2,\n",
    "    'size': (10.0, 10.0),\n",
    "    'start': (1.0,  5.0),\n",
    "    'goal': (8.0,  5.0)\n",
    "}\n",
    "\n",
    "deceleration = {\n",
    "    'center': (5.0, 5.0),\n",
    "    'decay': 2.0\n",
    "}\n",
    "\n",
    "# hyperparameters\n",
    "epoch = 300\n",
    "learning_rate = 0.001\n",
    "\n",
    "config = {\n",
    "    'MSE': {\n",
    "        'batch_size': 1000,\n",
    "        'max_time': 9,\n",
    "        'noise_generator': NoiseGenerator,\n",
    "        'loss_function': MSE_loss_function\n",
    "    },\n",
    "    'MSE_Lambda_same': {\n",
    "        'batch_size': 9,\n",
    "        'max_time': 9,\n",
    "        'noise_generator': PartialNoiseGeneratorSameTrajectory,\n",
    "        'loss_function': MSE_lambda_loss_function\n",
    "    },\n",
    "    'MSE_Lambda_not_same': {\n",
    "        'batch_size': 9,\n",
    "        'max_time': 9,\n",
    "        'noise_generator': PartialNoiseGeneratorDifferentTrajectory,\n",
    "        'loss_function': MSE_lambda_loss_function\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "for name, config in config.items():\n",
    "\n",
    "    batch_size = config['batch_size']\n",
    "    max_time   = config['max_time']\n",
    "\n",
    "    print(\">> Training {} ...\".format(name))\n",
    "    print(\">> batch_size =\", batch_size)\n",
    "    print(\">> max_time   =\", max_time)\n",
    "\n",
    "    graph = tf.Graph()\n",
    "\n",
    "    # MDP model\n",
    "    mdp = Navigation(graph, grid, deceleration)\n",
    "\n",
    "    # define policy network\n",
    "    layers = [mdp.state_size + 1, 20, 5, mdp.action_size]\n",
    "    policy = PolicyNetwork(graph, layers)\n",
    "\n",
    "    # unroll MDP model\n",
    "    rnn = MDP_RNN(mdp, policy, batch_size)\n",
    "    rewards, states, actions, final_state = rnn.unroll(max_time)\n",
    "\n",
    "    # inputs\n",
    "    noise_generator = config['noise_generator'](batch_size, max_time)\n",
    "\n",
    "    # initial state\n",
    "    x0, y0 = grid['start']\n",
    "    s0 = build_initial_state(x0, y0, batch_size)\n",
    "\n",
    "    # loss function\n",
    "    total, loss = config['loss_function'](graph, rewards)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = PolicyOptimizer(graph, loss, total, learning_rate, s0, noise_generator)\n",
    "\n",
    "    # results\n",
    "    losses, totals, saver, uptime = train(graph, optimizer, epoch)\n",
    "    print()\n",
    "\n",
    "    # simulations\n",
    "    print(\">> Simulating {}\".format(name))\n",
    "    \n",
    "    simulation_batch_size = 10000\n",
    "    simulation_max_time = 9\n",
    "    s0 = build_initial_state(x0, y0, simulation_batch_size)\n",
    "    _, _, _, total_cost = simulate(graph, [rewards, states, actions, total], s0, simulation_batch_size, simulation_max_time)\n",
    "    avg_total_cost = np.mean(total_cost)\n",
    "    metrics[name] = (losses, totals, avg_total_cost, uptime)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> SUMMARY:\")\n",
    "for name in sorted(metrics):\n",
    "    _, _, avg_total_cost, uptime = metrics[name]\n",
    "    print('{:25}: avg_total_cost = {:.4f}, uptime = {:4f} sec.'.format(name, avg_total_cost, uptime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss function and cost per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(metrics), ncols=2, figsize=(13, 8))\n",
    "fig.tight_layout(h_pad=5.0, w_pad=5.0)\n",
    "\n",
    "for i, name in enumerate(sorted(metrics)):\n",
    "    losses, totals, _, _ = metrics[name]\n",
    "\n",
    "    # plotting losses\n",
    "    ax = axes[i][0]\n",
    "    utils.plot_loss_function(ax, losses, epoch, name)\n",
    "\n",
    "    # histogram of cumulative cost per batch\n",
    "    ax = axes[i][1]\n",
    "    utils.plot_average_total_cost(ax, totals, epoch, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
