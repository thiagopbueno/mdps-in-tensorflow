{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDPs in TensorFlow - Navigation with Noisy Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this IPython notebook, we'll explore **Continuous State-Action MDPs** with stochastic transitions in TensorFlow. All stochastic transitions will be defined by a deterministic function combined with external noise that is considered an input to the MDP cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import abc\n",
    "import functools\n",
    "import time\n",
    "\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling MDPs in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes defining MDPs must inherit from abstract class ```MDP```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDP(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    @abc.abstractproperty\n",
    "    def action_size(self):\n",
    "        return\n",
    "    \n",
    "    @abc.abstractproperty\n",
    "    def state_size(self):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def transition(self, state, action):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def reward(self, state, action):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation in 2D grid with deceleration zone at the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Navigation(MDP):\n",
    "\n",
    "    def __init__(self, graph, grid, deceleration, max_theta=20):\n",
    "        self.graph = graph\n",
    "\n",
    "        self.ndim = grid[\"ndim\"]\n",
    "        self.max_theta = max_theta\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # grid constants\n",
    "            self.__size = tf.constant(grid[\"size\"], dtype=tf.float32)\n",
    "            self.__goal = tf.constant(grid[\"goal\"], dtype=tf.float32)\n",
    "\n",
    "            # deceleration constants\n",
    "            self.__center = tf.constant(deceleration[\"center\"], dtype=tf.float32)\n",
    "            self.__decay  = tf.constant(deceleration[\"decay\"],  dtype=tf.float32)\n",
    "\n",
    "            # numerical constants\n",
    "            self.__0_00 = tf.constant(0.00, dtype=tf.float32)\n",
    "            self.__1_00 = tf.constant(1.00, dtype=tf.float32)\n",
    "            self.__2_00 = tf.constant(2.00, dtype=tf.float32)\n",
    "            self.__8_00 = tf.constant(8.00, dtype=tf.float32)\n",
    "\n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.ndim\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.ndim\n",
    "        \n",
    "    def transition(self, state, action, noise):\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # apply rotation noise\n",
    "            cos = tf.cos(self.max_theta * np.pi / 180 * noise)\n",
    "            sin = tf.sin(self.max_theta * np.pi / 180 * noise)\n",
    "\n",
    "            noise_matrix = tf.stack([cos, -sin, sin, cos], axis=1)\n",
    "            noise_matrix = tf.reshape(noise_matrix, [-1, 2, 2])\n",
    "            noisy_action = tf.matmul(noise_matrix, tf.reshape(action, [-1, 2, 1]))\n",
    "            noisy_action = tf.reshape(noisy_action, [-1, 2])\n",
    "\n",
    "            # distance to center of deceleration zone\n",
    "            d = tf.sqrt(tf.reduce_sum(tf.square(state - self.__center), 1, keep_dims=True))\n",
    "\n",
    "            # deceleration_factor\n",
    "            deceleration = self.__2_00 / (self.__1_00 + tf.exp(-self.__decay * d)) - self.__1_00\n",
    "\n",
    "            # next position\n",
    "            next_state = state + deceleration * noisy_action\n",
    "            \n",
    "            # avoid getting out of map\n",
    "            next_state = tf.clip_by_value(next_state, self.__0_00, self.__size)\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reward(self, state, action):\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            # norm L-1 (manhattan distance)\n",
    "            # r = -tf.reduce_sum(tf.abs(state - self.__goal), 1, keep_dims=True)\n",
    "\n",
    "            # norm L-2 (euclidean distance)\n",
    "            r = -tf.sqrt(tf.reduce_sum(tf.square(state - self.__goal), 1, keep_dims=True))\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding an MDP as a Recurrent Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encapsulate MDP components into RNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDP_RNNCell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, mdp, policy):\n",
    "        self.mdp = mdp\n",
    "        self.policy = policy\n",
    "\n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.mdp.action_size\n",
    "        \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.mdp.state_size + 1\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.state_size + self.mdp.action_size + 1\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "\n",
    "        with self.mdp.graph.as_default():\n",
    "\n",
    "            # add policy network\n",
    "            mdp_action = self.policy(state)\n",
    "\n",
    "            # separate MDP state and timestep\n",
    "            h = tf.unstack(state, axis=1)\n",
    "            sz = self.mdp.state_size\n",
    "            mdp_state = tf.stack(h[:sz], axis=1)\n",
    "            timestep  = tf.reshape(h[sz], [-1, 1])\n",
    "\n",
    "            # add MDP components to the RNN cell output\n",
    "            noise = inputs\n",
    "            mdp_next_state = self.mdp.transition(mdp_state, mdp_action, noise)\n",
    "            mdp_reward = self.mdp.reward(mdp_next_state, mdp_action)\n",
    "\n",
    "            # gather MDP state and timestep\n",
    "            mdp_next_state = tf.concat([mdp_next_state, timestep + 1], axis=1)\n",
    "            \n",
    "            # concatenate outputs\n",
    "            outputs = tf.concat([mdp_reward, mdp_next_state, mdp_action], axis=1)\n",
    "\n",
    "        return outputs, mdp_next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the MDP's policy as a Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(object):\n",
    "    \n",
    "    def __init__(self, graph, layers, limits=1.0):\n",
    "        self.graph = graph\n",
    "        self.policy = functools.partial(self.__build_network, layers, limits)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        return self.policy(state)\n",
    "    \n",
    "    def __build_network(self, layers, limits, state):\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            with tf.variable_scope('policy'):\n",
    "\n",
    "                # hidden layers\n",
    "                outputs = state\n",
    "                for i, n_h in enumerate(layers[1:]):\n",
    "                    if i != len(layers)-2:\n",
    "                        activation = tf.nn.relu\n",
    "                    else:\n",
    "                        activation = tf.nn.tanh\n",
    "\n",
    "                    outputs = tf.layers.dense(outputs,\n",
    "                                              units=n_h,\n",
    "                                              activation=activation,\n",
    "                                              kernel_initializer=tf.glorot_normal_initializer(),\n",
    "                                              name=\"layer\"+str(i+1))\n",
    "\n",
    "                # add action limits over last tanh layer\n",
    "                action = tf.constant(limits) * outputs\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unroll the model given a finite horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDP_RNN(object):\n",
    "    \n",
    "    def __init__(self, mdp, policy, batch_size=1):\n",
    "        self.cell = MDP_RNNCell(mdp, policy)\n",
    "        self.batch_size = batch_size\n",
    "        self.graph = mdp.graph\n",
    "    \n",
    "    def unroll(self, inputs, initial_state):\n",
    "\n",
    "        state_size = self.cell.state_size\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # dynamic time unrolling\n",
    "            outputs, final_state = tf.nn.dynamic_rnn(\n",
    "                self.cell,\n",
    "                inputs,\n",
    "                initial_state=initial_state,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            # gather reward, state and action series\n",
    "            outputs = tf.unstack(outputs, axis=2)\n",
    "            max_time = int(inputs.shape[1])\n",
    "            reward_series = tf.reshape(outputs[0], [-1, max_time, 1])\n",
    "            state_series  = tf.stack(outputs[1:1+state_size], axis=2)\n",
    "            action_series = tf.stack(outputs[1+state_size:],  axis=2)\n",
    "        \n",
    "        return reward_series, state_series, action_series, final_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Policy Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyOptimizer(object):\n",
    "    \n",
    "    def __init__(self, graph, loss, total, learning_rate, initial_state, noise_generator):\n",
    "        self.graph = graph\n",
    "\n",
    "        self.loss = loss\n",
    "        self.total = total\n",
    "        \n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "        self.noise = noise_generator\n",
    "        \n",
    "        # optimization hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            # backprop via RMSProp\n",
    "            self.train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "            # global initializer\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def run(self, sess, epoch=100, show_progress=True):\n",
    "        # initialize variables\n",
    "        sess.run(self.init_op)\n",
    "        \n",
    "        losses = []\n",
    "        for epoch_idx in range(epoch):\n",
    "            # generate inputs (noise)\n",
    "            inputs_data = self.noise()\n",
    "\n",
    "            # backprop and update weights\n",
    "            _, loss, total = sess.run([self.train_step, self.loss, self.total],\n",
    "                                      feed_dict={inputs: inputs_data, initial_state: self.initial_state})\n",
    "\n",
    "            # store and show loss information\n",
    "            losses.append(loss)\n",
    "            if show_progress:\n",
    "                print('Epoch {0:5}: loss = {1}\\r'.format(epoch_idx, loss), end='')\n",
    "        print()\n",
    "\n",
    "        return losses, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the noise generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NoiseGenerator(object):\n",
    "    \n",
    "    def __init__(self, batch_size, max_time, ratio):\n",
    "        self.size = (batch_size, max_time, 1)\n",
    "        self.noise_size = (int(ratio * batch_size), max_time, 1)\n",
    "\n",
    "        # no noise at all...\n",
    "        shape = (batch_size - self.noise_size[0], max_time, 1)\n",
    "        self.zero_noise = np.zeros(shape=shape).astype(np.float32)\n",
    "\n",
    "    def __call__(self):\n",
    "        random_noise = np.random.normal(size=self.noise_size).astype(np.float32)\n",
    "        return np.concatenate([random_noise, self.zero_noise], axis=0)\n",
    "    \n",
    "class PartialNoiseGenerator(object):\n",
    "\n",
    "    def __init__(self, batch_size, max_time, same=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_time = max_time\n",
    "        self.same = same\n",
    "\n",
    "    def __call__(self):\n",
    "        if self.same:\n",
    "            noise = np.random.normal(size=(1, self.max_time)).astype(np.float32)\n",
    "            noise = np.repeat(noise, [self.batch_size], axis=0)\n",
    "        else:\n",
    "            noise = np.random.normal(size=(self.batch_size, self.max_time)).astype(np.float32)\n",
    "\n",
    "        noise = np.tril(noise)\n",
    "        noise = np.reshape(noise, noise.shape + (1,))\n",
    "\n",
    "        return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all components together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first of all create the computational graph to which all necessary operations will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the MDP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# navigation parameters\n",
    "grid = {\n",
    "    'ndim': 2,\n",
    "    'size': (10.0, 10.0),\n",
    "    'start': (1.0,  5.0),\n",
    "    'goal': (8.0,  5.0)\n",
    "}\n",
    "\n",
    "deceleration = {\n",
    "    'center': (5.0, 5.0),\n",
    "    'decay': 2.0\n",
    "}\n",
    "\n",
    "# MDP model\n",
    "mdp = Navigation(graph, grid, deceleration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define policy network\n",
    "layers = [mdp.state_size + 1, 10, 5, mdp.action_size]\n",
    "policy = PolicyNetwork(graph, layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unroll the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 9\n",
    "max_time = 9\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # initial state\n",
    "    initial_state = tf.placeholder(tf.float32, shape=(None, mdp.state_size + 1), name=\"initial_state\")\n",
    "\n",
    "    # inputs (noise)\n",
    "    inputs = tf.placeholder(tf.float32, shape=(None, max_time, 1), name=\"inputs\")\n",
    "\n",
    "# unroll MDP model\n",
    "rnn = MDP_RNN(mdp, policy, batch_size)\n",
    "rewards, states, actions, final_state = rnn.unroll(inputs, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace-decay parameter lambda for controlling the bootstrap\n",
    "lambda_discount_factor = 0.9 \n",
    "lambda_factor = np.full(batch_size,\n",
    "                        (1 - lambda_discount_factor) / (1 - lambda_discount_factor ** batch_size),\n",
    "                        dtype=np.float32)\n",
    "l = 1\n",
    "for i in range(len(lambda_factor)):\n",
    "    lambda_factor[i] *= l\n",
    "    l *= lambda_discount_factor\n",
    "# lambda_factor = np.append(lambda_factor, [1 - np.sum(lambda_factor)])\n",
    "lambda_factor = np.reshape(lambda_factor, (batch_size, 1))\n",
    "\n",
    "# print(np.sum(lambda_factor))\n",
    "# print(lambda_factor.shape)\n",
    "# print(lambda_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "\n",
    "    # MSE loss function\n",
    "    # total = tf.reduce_sum(rewards, axis=1)\n",
    "    # loss  = tf.reduce_mean(tf.square(total))\n",
    "\n",
    "    # average TD(lambda) loss funciton\n",
    "    total = tf.reduce_sum(rewards, axis=1) * tf.constant(lambda_factor)\n",
    "    loss  = tf.reduce_sum(tf.square(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(graph, optimizer, epoch):\n",
    "\n",
    "    # saver\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # optimize it, babe!\n",
    "        losses, total_cost_per_batch = optimizer.run(sess, epoch)\n",
    "\n",
    "        end = time.time()\n",
    "        uptime = end - start\n",
    "        print(\"Done in {0:.6f} sec.\\n\".format(uptime))\n",
    "\n",
    "        # save model\n",
    "        save_path = saver.save(sess, 'models/model.ckpt')\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    return losses, total_cost_per_batch, saver, uptime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the initial state for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_initial_state(x0, y0, batch_size):\n",
    "    x_init = np.full([batch_size], x0, np.float32)\n",
    "    y_init = np.full([batch_size], y0, np.float32)\n",
    "    t_init = np.zeros([batch_size], np.float32)\n",
    "    return np.stack([x_init, y_init, t_init], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "epoch = 200\n",
    "learning_rate = 0.005\n",
    "\n",
    "# inputs\n",
    "\n",
    "# regular random noise\n",
    "# noise_ratio = 1.00\n",
    "# noise_generator = NoiseGenerator(batch_size, max_time, noise_ratio)\n",
    "\n",
    "# TD noise generator\n",
    "noise_generator = PartialNoiseGenerator(batch_size, max_time)\n",
    "\n",
    "# initial state\n",
    "x0, y0 = grid['start']\n",
    "s0 = build_initial_state(x0, y0, batch_size)\n",
    "\n",
    "# optimizer\n",
    "optimizer = PolicyOptimizer(graph, loss, total, learning_rate, s0, noise_generator)\n",
    "\n",
    "# results\n",
    "losses, total_cost, saver, uptime = train(graph, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss function and cost per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# plotting losses\n",
    "ax = fig.add_subplot(121)\n",
    "utils.plot_loss_function(ax, losses, epoch)\n",
    "\n",
    "# histogram of cumulative cost per batch\n",
    "ax = fig.add_subplot(122)\n",
    "utils.plot_total_cost_per_batch(ax, total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the learned policy in an action grid for different timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_initial_states_grid(x_grid_size, y_grid_size, timestep):\n",
    "    batch_size = x_grid_size * y_grid_size\n",
    "    x_grid = np.linspace(0.0, grid['size'][0], x_grid_size)\n",
    "    y_grid = np.linspace(0.0, grid['size'][1], y_grid_size)\n",
    "    initial_states_grid = []\n",
    "    for x in x_grid:\n",
    "        for y in y_grid:\n",
    "            initial_states_grid.append([x, y, timestep])\n",
    "    return initial_states_grid, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(grid_size, state_size, timesteps):\n",
    "    initial_states_grids = []\n",
    "    actions = []\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        \n",
    "        # re-initialize initial_state placeholder\n",
    "        batch_size = grid_size * grid_size\n",
    "        initial_state = tf.placeholder(shape=(batch_size, state_size), dtype=np.float32, name='initial_state')\n",
    "\n",
    "        # re-initialize policy\n",
    "        with tf.variable_scope('rnn'):\n",
    "            policy = PolicyNetwork(graph, layers)\n",
    "            action = policy(initial_state)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            saver.restore(sess, 'models/model.ckpt')\n",
    "            \n",
    "            for timestep in timesteps:\n",
    "\n",
    "                # instantiate initial states in a grid \n",
    "                s0, batch_size = build_initial_states_grid(grid_size, grid_size, timestep)\n",
    "                initial_states_grids.append(s0)\n",
    "\n",
    "                # evaluate policy for given initial states\n",
    "                a = sess.run(action, feed_dict={initial_state: s0})\n",
    "                actions.append(a)\n",
    "    \n",
    "    return initial_states_grids, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "timesteps = np.array([0.0, max_time / 3, 2 / 3 * max_time, max_time], dtype=np.int32)\n",
    "initial_states_grid, policy_actions = evaluate_policy(grid_size, mdp.state_size + 1, timesteps)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "for i, timestep in enumerate(timesteps):\n",
    "    ax = fig.add_subplot(1, len(timesteps), i+1)\n",
    "    utils.plot_policy(ax, grid, deceleration, initial_states_grid[i], policy_actions[i], timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate(graph, series, s0, batch_size=1, max_time=10):\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # restore learned policy model\n",
    "        saver.restore(sess, 'models/model.ckpt')\n",
    "\n",
    "        # sample noise data\n",
    "        noise = np.random.normal(size=(batch_size, max_time, 1)).astype(np.float32)\n",
    "\n",
    "        # simulate MDP trajectories\n",
    "        result = sess.run(series, feed_dict={inputs: noise, initial_state: s0})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "\n",
    "# initial states for simulation\n",
    "x0, y0 = grid['start']\n",
    "delta_y = [0.0, -1.0, -1.5, 1.5, 1.0]\n",
    "initial_states = []\n",
    "for delta in delta_y:\n",
    "    s0 = build_initial_state(x0, y0 + delta, batch_size)\n",
    "    initial_states.append(s0)\n",
    "initial_states = np.concatenate(initial_states, axis=0)\n",
    "\n",
    "# total batch_size\n",
    "batch_size = initial_states.shape[0]\n",
    "\n",
    "# simulate!\n",
    "rewards, states, actions = simulate(graph, [rewards, states, actions], initial_states, batch_size, max_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot simulated trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 25))\n",
    "utils.plot_simulations(fig, grid, deceleration, initial_states, delta_y, states, actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
