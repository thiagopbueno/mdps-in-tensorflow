{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDPs in TensorFlow - Navigation with Noisy Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this IPython notebook, we'll explore **Continuous State-Action MDPs** with stochastic transitions in TensorFlow. All stochastic transitions will be defined by a deterministic function combined with external noise that is considered an input to the MDP cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import abc\n",
    "import functools\n",
    "import time\n",
    "\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling MDPs in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes defining MDPs must inherit from abstract class ```MDP```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDP(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    @abc.abstractproperty\n",
    "    def action_size(self):\n",
    "        return\n",
    "    \n",
    "    @abc.abstractproperty\n",
    "    def state_size(self):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def transition(self, state, action):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def reward(self, state, action):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation in 2D grid with deceleration zone at the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Navigation(MDP):\n",
    "\n",
    "    def __init__(self, graph, grid, deceleration, max_theta=20):\n",
    "        self.graph = graph\n",
    "\n",
    "        self.ndim = grid[\"ndim\"]\n",
    "        self.max_theta = max_theta\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # grid constants\n",
    "            self.__size = tf.constant(grid[\"size\"], dtype=tf.float32)\n",
    "            self.__goal = tf.constant(grid[\"goal\"], dtype=tf.float32)\n",
    "\n",
    "            # deceleration constants\n",
    "            self.__center = tf.constant(deceleration[\"center\"], dtype=tf.float32)\n",
    "            self.__decay  = tf.constant(deceleration[\"decay\"],  dtype=tf.float32)\n",
    "\n",
    "            # numerical constants\n",
    "            self.__0_00 = tf.constant(0.00, dtype=tf.float32)\n",
    "            self.__1_00 = tf.constant(1.00, dtype=tf.float32)\n",
    "            self.__2_00 = tf.constant(2.00, dtype=tf.float32)\n",
    "            self.__8_00 = tf.constant(8.00, dtype=tf.float32)\n",
    "\n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.ndim\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.ndim\n",
    "        \n",
    "    def transition(self, state, action, noise):\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # apply rotation noise\n",
    "            cos = tf.cos(self.max_theta * np.pi / 180 * noise)\n",
    "            sin = tf.sin(self.max_theta * np.pi / 180 * noise)\n",
    "\n",
    "            noise_matrix = tf.stack([cos, -sin, sin, cos], axis=1)\n",
    "            noise_matrix = tf.reshape(noise_matrix, [-1, 2, 2])\n",
    "            noisy_action = tf.matmul(noise_matrix, tf.reshape(action, [-1, 2, 1]))\n",
    "            noisy_action = tf.reshape(noisy_action, [-1, 2])\n",
    "\n",
    "            # distance to center of deceleration zone\n",
    "            d = tf.sqrt(tf.reduce_sum(tf.square(state - self.__center), 1, keep_dims=True))\n",
    "\n",
    "            # deceleration_factor\n",
    "            deceleration = self.__2_00 / (self.__1_00 + tf.exp(-self.__decay * d)) - self.__1_00\n",
    "\n",
    "            # next position\n",
    "            next_state = state + deceleration * noisy_action\n",
    "            \n",
    "            # avoid getting out of map\n",
    "            next_state = tf.clip_by_value(next_state, self.__0_00, self.__size)\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reward(self, state, action):\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            # norm L-1 (manhattan distance)\n",
    "            # r = -tf.reduce_sum(tf.abs(state - self.__goal), 1, keep_dims=True)\n",
    "\n",
    "            # norm L-2 (euclidean distance)\n",
    "            r = -tf.sqrt(tf.reduce_sum(tf.square(state - self.__goal), 1, keep_dims=True))\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding an MDP as a Recurrent Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encapsulate MDP components into RNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDP_RNNCell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, mdp, policy):\n",
    "        self.mdp = mdp\n",
    "        self.policy = policy\n",
    "\n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.mdp.action_size\n",
    "        \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.mdp.state_size + 1\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.state_size + self.mdp.action_size + 1\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "\n",
    "        with self.mdp.graph.as_default():\n",
    "\n",
    "            # add policy network\n",
    "            mdp_action = self.policy(state)\n",
    "\n",
    "            # separate MDP state and timestep\n",
    "            h = tf.unstack(state, axis=1)\n",
    "            sz = self.mdp.state_size\n",
    "            mdp_state = tf.stack(h[:sz], axis=1)\n",
    "            timestep  = tf.reshape(h[sz], [int(state.shape[0]), 1])\n",
    "\n",
    "            # add MDP components to the RNN cell output\n",
    "            noise = inputs\n",
    "            mdp_next_state = self.mdp.transition(mdp_state, mdp_action, noise)\n",
    "            mdp_reward = self.mdp.reward(mdp_next_state, mdp_action)\n",
    "\n",
    "            # gather MDP state and timestep\n",
    "            mdp_next_state = tf.concat([mdp_next_state, timestep + 1], axis=1)\n",
    "            \n",
    "            # concatenate outputs\n",
    "            outputs = tf.concat([mdp_reward, mdp_next_state, mdp_action], axis=1)\n",
    "\n",
    "        return outputs, mdp_next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the MDP's policy as a Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(object):\n",
    "    \n",
    "    def __init__(self, graph, layers, limits=1.0):\n",
    "        self.graph = graph\n",
    "        self.policy = functools.partial(self.__build_network, layers, limits)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        return self.policy(state)\n",
    "    \n",
    "    def __build_network(self, layers, limits, state):\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            with tf.variable_scope('policy'):\n",
    "\n",
    "                # hidden layers\n",
    "                outputs = state\n",
    "                for i, n_h in enumerate(layers[1:]):\n",
    "                    if i != len(layers)-2:\n",
    "                        activation = tf.nn.relu\n",
    "                    else:\n",
    "                        activation = tf.nn.tanh\n",
    "\n",
    "                    outputs = tf.layers.dense(outputs,\n",
    "                                              units=n_h,\n",
    "                                              activation=activation,\n",
    "                                              kernel_initializer=tf.glorot_normal_initializer(),\n",
    "                                              name=\"layer\"+str(i+1))\n",
    "\n",
    "                # add action limits over last tanh layer\n",
    "                action = tf.constant(limits) * outputs\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unroll the model given a finite horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDP_RNN(object):\n",
    "    \n",
    "    def __init__(self, mdp, policy, batch_size=1):\n",
    "        self.cell = MDP_RNNCell(mdp, policy)\n",
    "        self.batch_size = batch_size\n",
    "        self.graph = mdp.graph\n",
    "    \n",
    "    def unroll(self, inputs, initial_state):\n",
    "\n",
    "        state_size = self.cell.state_size\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # dynamic time unrolling\n",
    "            outputs, final_state = tf.nn.dynamic_rnn(\n",
    "                self.cell,\n",
    "                inputs,\n",
    "                initial_state=initial_state,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            # gather reward, state and action series\n",
    "            outputs = tf.unstack(outputs, axis=2)\n",
    "            max_time = int(inputs.shape[1])\n",
    "            reward_series = tf.reshape(outputs[0], [-1, max_time, 1])\n",
    "            state_series  = tf.stack(outputs[1:1+state_size], axis=2)\n",
    "            action_series = tf.stack(outputs[1+state_size:],  axis=2)\n",
    "        \n",
    "        return reward_series, state_series, action_series, final_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Policy Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyOptimizer(object):\n",
    "    \n",
    "    def __init__(self, graph, loss, total, learning_rate, noise_generator):\n",
    "        self.graph = graph\n",
    "\n",
    "        self.loss = loss\n",
    "        self.total = total\n",
    "        \n",
    "        self.noise = noise_generator\n",
    "        \n",
    "        # optimization hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            # backprop via RMSProp\n",
    "            self.train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "            # global initializer\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def run(self, sess, epoch=100, show_progress=True):\n",
    "        # initialize variables\n",
    "        sess.run(self.init_op)\n",
    "        \n",
    "        losses = []\n",
    "        for epoch_idx in range(epoch):\n",
    "            # generate inputs (noise)\n",
    "            inputs_data = self.noise()\n",
    "\n",
    "            # backprop and update weights\n",
    "            _, loss, total = sess.run([self.train_step, self.loss, self.total], feed_dict={inputs: inputs_data})\n",
    "\n",
    "            # store and show loss information\n",
    "            losses.append(loss)\n",
    "            if show_progress:\n",
    "                print('Epoch {0:5}: loss = {1}\\r'.format(epoch_idx, loss), end='')\n",
    "        print()\n",
    "\n",
    "        return losses, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the noise generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NoiseGenerator(object):\n",
    "    \n",
    "    def __init__(self, batch_size, max_time, ratio):\n",
    "        self.size = (batch_size, max_time, 1)\n",
    "        self.noise_size = (int(ratio * batch_size), max_time, 1)\n",
    "\n",
    "        # no noise at all...\n",
    "        shape = (batch_size - self.noise_size[0], max_time, 1)\n",
    "        self.zero_noise = np.zeros(shape=shape).astype(np.float32)\n",
    "\n",
    "    def __call__(self):\n",
    "        # sample noise data from normal distribution\n",
    "        random_noise = np.random.normal(size=self.noise_size).astype(np.float32)\n",
    "        return np.concatenate([random_noise, self.zero_noise], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all components together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first of all create the computational graph to which all necessary operations will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the MDP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# navigation parameters\n",
    "grid = {\n",
    "    'ndim': 2,\n",
    "    'size': (10.0, 10.0),\n",
    "    'initial': (1.0,  5.0),\n",
    "    'goal': (8.0,  5.0)\n",
    "}\n",
    "\n",
    "deceleration = {\n",
    "    'center': (5.0, 5.0),\n",
    "    'decay': 2.0\n",
    "}\n",
    "\n",
    "# MDP model\n",
    "mdp = Navigation(graph, grid, deceleration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define policy network\n",
    "layers = [mdp.state_size + 1, 10, 5, mdp.action_size]\n",
    "policy = PolicyNetwork(graph, layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unroll the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "max_time = 9\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # same initial state for each batch\n",
    "    x_initial, y_initial = grid['initial']\n",
    "    x_initial = tf.fill([batch_size], tf.constant(x_initial, tf.float32))\n",
    "    y_initial = tf.fill([batch_size], tf.constant(y_initial, tf.float32))\n",
    "    timestep_initial = tf.fill([batch_size], tf.constant(0.0, tf.float32))\n",
    "    initial_state = tf.stack([x_initial, y_initial, timestep_initial], axis=1)\n",
    "\n",
    "    # initial state\n",
    "#     initial_state = tf.placeholder(tf.float32,\n",
    "#                                    shape=(batch_size, max_time, mdp.state_size + 1),\n",
    "#                                    name=\"initial_state\")\n",
    "\n",
    "    # inputs (noise)\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, max_time, 1], name=\"inputs\")\n",
    "\n",
    "# unroll MDP model\n",
    "rnn = MDP_RNN(mdp, policy, batch_size)\n",
    "rewards, states, actions, final_state = rnn.unroll(inputs, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # loss based on total reward\n",
    "    total = tf.reduce_sum(rewards, 1)\n",
    "    loss  = tf.reduce_mean(tf.square(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(graph, optimizer, epoch):\n",
    "\n",
    "    # saver\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # optimize it, babe!\n",
    "        losses, total_cost_per_batch = optimizer.run(sess, epoch)\n",
    "\n",
    "        end = time.time()\n",
    "        uptime = end - start\n",
    "        print(\"Done in {0:.6f} sec.\\n\".format(uptime))\n",
    "\n",
    "        # save model\n",
    "        save_path = saver.save(sess, 'models/model.ckpt')\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    return losses, total_cost_per_batch, saver, uptime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "epoch = 300\n",
    "learning_rate = 0.001\n",
    "\n",
    "# optimizer\n",
    "noise_ratio = 1.00\n",
    "noise_generator = NoiseGenerator(batch_size, max_time, noise_ratio)\n",
    "optimizer = PolicyOptimizer(graph, loss, total, learning_rate, noise_generator)\n",
    "\n",
    "# results\n",
    "losses, total_cost, saver, uptime = train(graph, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss function and cost per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# plotting losses\n",
    "ax = fig.add_subplot(121)\n",
    "utils.plot_loss_function(ax, losses, epoch)\n",
    "\n",
    "# histogram of cumulative cost per batch\n",
    "ax = fig.add_subplot(122)\n",
    "utils.plot_total_cost_per_batch(ax, total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_initial_states_grid(x_grid_size, y_grid_size, timestep):\n",
    "    batch_size = x_grid_size * y_grid_size\n",
    "    x_grid = np.linspace(0.0, grid['size'][0], x_grid_size)\n",
    "    y_grid = np.linspace(0.0, grid['size'][1], y_grid_size)\n",
    "    initial_states_grid = []\n",
    "    for x in x_grid:\n",
    "        for y in y_grid:\n",
    "            initial_states_grid.append([x, y, timestep])\n",
    "    return initial_states_grid, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(grid_size, timestep):\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        initial_states_grid, batch_size = build_initial_states_grid(grid_size, grid_size, timestep)\n",
    "        initial_state = tf.constant(initial_states_grid)\n",
    "\n",
    "        with tf.variable_scope('rnn'):\n",
    "            policy = PolicyNetwork(graph, layers)\n",
    "            action = policy(initial_state)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            saver.restore(sess, 'models/model.ckpt')\n",
    "            actions = sess.run(action)\n",
    "    \n",
    "    return initial_states_grid, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot policy across different time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "timesteps = [0.0, max_time/3, 2*max_time/3, max_time]\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "for i, timestep in enumerate(timesteps):\n",
    "    ax = fig.add_subplot(1, len(timesteps), i+1)\n",
    "    initial_states_grid, actions = evaluate_policy(grid_size, int(timestep))\n",
    "    utils.plot_policy(ax, grid, deceleration, initial_states_grid, actions, int(timestep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate(graph, series, batch_size=1, max_time=10):\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # restore learned policy model\n",
    "        saver.restore(sess, 'models/model.ckpt')\n",
    "\n",
    "        # sample noise data\n",
    "        inputs_data = np.random.normal(size=(batch_size, max_time, 1)).astype(np.float32)\n",
    "\n",
    "        # simulate MDP trajectories\n",
    "        result = sess.run(series, feed_dict={inputs: inputs_data})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "max_time = 9\n",
    "batch_size = 3\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # initial states for simulation\n",
    "    x_start, y_start = grid['initial']\n",
    "    x_initial = tf.fill([batch_size], tf.constant(x_start, tf.float32))\n",
    "    y_initial = tf.fill([batch_size], tf.constant(y_start, tf.float32))\n",
    "    timestep_initial = tf.fill([batch_size], tf.constant(0.0, tf.float32))\n",
    "    initial_state = tf.stack([x_initial, y_initial, timestep_initial], axis=1)\n",
    "    delta = [-3.0, -1.5, 1.5, 3.0]\n",
    "    for delta_y in delta:\n",
    "        x_initial = tf.fill([batch_size], tf.constant(x_start, tf.float32))\n",
    "        y_initial = tf.fill([batch_size], tf.constant(y_start + delta_y, tf.float32))\n",
    "        timestep_initial = tf.fill([batch_size], tf.constant(0.0, tf.float32))\n",
    "        initial_state = tf.concat([initial_state, tf.stack([x_initial, y_initial, timestep_initial], axis=1)], axis=0)\n",
    "\n",
    "    batch_size = initial_state.shape[0]\n",
    "\n",
    "    # inputs\n",
    "    inputs = tf.placeholder(tf.float32, shape=[batch_size, max_time, 1], name=\"inputs\")\n",
    "\n",
    "    # MDP model\n",
    "    mdp = Navigation(graph, grid, deceleration)\n",
    "\n",
    "    # unroll MDP model\n",
    "    policy = PolicyNetwork(graph, layers)\n",
    "    rnn = MDP_RNN(mdp, policy, batch_size)\n",
    "    rewards, states, actions, final_state = rnn.unroll(inputs, initial_state)\n",
    "\n",
    "    # simulate\n",
    "    rewards, states, actions = simulate(graph, [rewards, states, actions], batch_size, max_time)\n",
    "    \n",
    "    # check timestep\n",
    "    for batch in states:\n",
    "        for i, state in enumerate(batch):\n",
    "            assert(int(state[2]) == i+1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot simulated trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = grid['initial'], grid['goal']\n",
    "fig = plt.figure(figsize=(15, 25))\n",
    "num_plots = int(initial_state.shape[0])\n",
    "deltas = [0] + delta\n",
    "rows = len(deltas)\n",
    "cols = num_plots // len(deltas)\n",
    "for i in range(num_plots):\n",
    "    ax = fig.add_subplot(len(deltas),num_plots/len(deltas),i+1)\n",
    "    idx = i//cols\n",
    "    start = (grid['initial'][0], grid['initial'][1] + deltas[idx])\n",
    "    utils.plot_simulation(ax, grid, deceleration, start, end, states[i], actions[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
