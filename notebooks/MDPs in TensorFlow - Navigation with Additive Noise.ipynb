{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDPs in TensorFlow - Navigation with Additive Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this IPython notebook, we'll explore how to model in TensorFlow a **Continuous State-Action MDP** with stochastic transitions defined by additive noise, i.e.:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{s}_{t+1} &= \\mathcal{T}(\\mathbf{s}_t, \\mathbf{a}_{t+1}) + \\xi \\\\\n",
    "\\xi &\\sim p(\\cdot; \\mathbf{s}_t, \\mathbf{a}_{t+1})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\xi$ is a continuos random variable distributed as the probability density function $p(\\xi; \\mathbf{s}_t, \\mathbf{a}_{t+1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import abc\n",
    "import functools\n",
    "import time\n",
    "\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling MDPs in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes defining MDPs must inherit from abstract class ```MDP```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    @abc.abstractproperty\n",
    "    def action_size(self):\n",
    "        return\n",
    "    \n",
    "    @abc.abstractproperty\n",
    "    def state_size(self):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def transition(self, state, action, noise=None):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def reward(self, state, action):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation in 2D grid with deceleration zone at the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Navigation(MDP):\n",
    "\n",
    "    def __init__(self, graph, grid, deceleration):\n",
    "        self.graph = graph\n",
    "\n",
    "        self.ndim = grid[\"ndim\"]\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # grid constants\n",
    "            self.__size = tf.constant(grid[\"size\"], dtype=tf.float32)\n",
    "            self.__goal = tf.constant(grid[\"goal\"], dtype=tf.float32)\n",
    "\n",
    "            # deceleration constants\n",
    "            self.__center = tf.constant(deceleration[\"center\"], dtype=tf.float32)\n",
    "            self.__decay  = tf.constant(deceleration[\"decay\"],  dtype=tf.float32)\n",
    "\n",
    "            # numerical constants\n",
    "            self.__0_00 = tf.constant(0.00, dtype=tf.float32)\n",
    "            self.__1_00 = tf.constant(1.00, dtype=tf.float32)\n",
    "            self.__2_00 = tf.constant(2.00, dtype=tf.float32)\n",
    "\n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.ndim\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.ndim\n",
    "        \n",
    "    def transition(self, state, action):\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # distance to center of deceleration zone\n",
    "            d = tf.sqrt(tf.reduce_sum(tf.square(state - self.__center), axis=1, keep_dims=True))\n",
    "\n",
    "            # deceleration_factor\n",
    "            deceleration = self.__2_00 / (self.__1_00 + tf.exp(-self.__decay * d)) - self.__1_00\n",
    "\n",
    "            # deterministic next position\n",
    "            p = state + deceleration * action\n",
    "\n",
    "            # noise (with reparameterization trick)\n",
    "            noise = tf.distributions.Normal(loc=p, scale=0.2)\n",
    "\n",
    "            # next state\n",
    "            next_state = noise.sample()\n",
    "\n",
    "            # avoid getting out of map\n",
    "            next_state = tf.clip_by_value(next_state, self.__0_00, self.__size)\n",
    "\n",
    "        return next_state, noise.log_prob(next_state)\n",
    "\n",
    "    def reward(self, state, action):\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # norm L-2 (euclidean distance)\n",
    "            r = -tf.sqrt(tf.reduce_sum(tf.square(state - self.__goal), axis=1, keep_dims=True))\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding an MDP as a Recurrent Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encapsulate MDP components into RNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP_RNNCell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, mdp, policy):\n",
    "        self.mdp = mdp\n",
    "        self.policy = policy\n",
    "\n",
    "    @property\n",
    "    def action_size(self):\n",
    "        return self.mdp.action_size\n",
    "        \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.mdp.state_size * 2\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.mdp.state_size + self.mdp.action_size + 1\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "\n",
    "        # timestep\n",
    "        timestep = inputs\n",
    "        \n",
    "        # separate state and the history likelihood\n",
    "        state = tf.unstack(state, axis=1)\n",
    "        state, likelihood = state[:self.state_size//2], state[self.state_size//2:]\n",
    "        state = tf.stack(state, axis=1)\n",
    "        likelihood = tf.stack(likelihood, axis=1)\n",
    "        \n",
    "        with self.mdp.graph.as_default():\n",
    "\n",
    "            # augment state by adding timestep to state vector\n",
    "            state_t = tf.concat([state, timestep], axis=1)\n",
    "\n",
    "            # add policy network with augmented state as input\n",
    "            action = self.policy(state_t)\n",
    "\n",
    "            # add MDP components to the RNN cell output\n",
    "            next_state, log_prob = self.mdp.transition(state, action)\n",
    "            reward = self.mdp.reward(next_state, action)\n",
    "            \n",
    "            # combine rewards and likelihood for surrogate loss function\n",
    "            likelihood += log_prob\n",
    "            # reward += likelihood * reward\n",
    "\n",
    "            # concatenate outputs\n",
    "            outputs = tf.concat([reward, next_state, action], axis=1)\n",
    "\n",
    "            # concatenate togeter next state and updated likelihood of history so far\n",
    "            next_state = tf.concat([next_state, likelihood], axis=1)\n",
    "            \n",
    "        return outputs, next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the MDP's policy as a Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(object):\n",
    "    \n",
    "    def __init__(self, graph, layers, limits=1.0):\n",
    "        self.graph = graph\n",
    "        self.policy = functools.partial(self.__build_network, layers, limits)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        return self.policy(state)\n",
    "    \n",
    "    def __build_network(self, layers, limits, state):\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            with tf.variable_scope('policy'):\n",
    "\n",
    "                # hidden layers\n",
    "                outputs = state\n",
    "                for i, n_h in enumerate(layers[1:]):\n",
    "                    if i != len(layers)-2:\n",
    "                        activation = tf.nn.relu\n",
    "                    else:\n",
    "                        activation = tf.nn.tanh\n",
    "\n",
    "                    outputs = tf.layers.dense(outputs,\n",
    "                                              units=n_h,\n",
    "                                              activation=activation,\n",
    "                                              kernel_initializer=tf.glorot_normal_initializer(),\n",
    "                                              name=\"layer\"+str(i+1))\n",
    "\n",
    "                # add action limits over last tanh layer\n",
    "                action = tf.constant(limits) * outputs\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unroll the model given a finite horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP_RNN(object):\n",
    "    \n",
    "    def __init__(self, mdp, policy, batch_size=1):\n",
    "        self.cell = MDP_RNNCell(mdp, policy)\n",
    "        self.graph = mdp.graph\n",
    "    \n",
    "    def unroll(self, initial_state, timesteps):\n",
    "\n",
    "        inputs = timesteps\n",
    "\n",
    "        max_time = int(inputs.shape[1])\n",
    "        state_size = self.cell.state_size // 2\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            \n",
    "            # timesteps\n",
    "            inputs = tf.placeholder_with_default(tf.constant(timesteps, name='timesteps'),\n",
    "                                                 shape=(None, max_time, 1),\n",
    "                                                 name='inputs')\n",
    "            # initial cell state\n",
    "            initial_state = tf.placeholder_with_default(tf.constant(initial_state),\n",
    "                                                        shape=(None, self.cell.state_size),\n",
    "                                                        name='initial_state')\n",
    "\n",
    "            # dynamic time unrolling\n",
    "            outputs, final_state = tf.nn.dynamic_rnn(\n",
    "                self.cell,\n",
    "                inputs,\n",
    "                initial_state=initial_state,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            # gather reward, state and action series\n",
    "            outputs = tf.unstack(outputs, axis=2)\n",
    "            reward_series = tf.reshape(outputs[0], [-1, max_time, 1])\n",
    "            state_series  = tf.stack(outputs[1:1+state_size], axis=2)\n",
    "            action_series = tf.stack(outputs[1+state_size:],  axis=2)\n",
    "        \n",
    "        return reward_series, state_series, action_series, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Policy Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyOptimizer(object):\n",
    "    \n",
    "    def __init__(self, graph, loss, total, learning_rate):\n",
    "        self.graph = graph\n",
    "\n",
    "        # performance metrics\n",
    "        self.loss = loss\n",
    "        self.total = total\n",
    "\n",
    "        # optimization hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # backprop via RMSProp\n",
    "            self.train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "            # global initializer\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def run(self, sess, epoch=100, show_progress=True):\n",
    "        \n",
    "        # initialize variables\n",
    "        sess.run(self.init_op)\n",
    "        \n",
    "        losses = []\n",
    "        totals = []\n",
    "        for epoch_idx in range(epoch):\n",
    "\n",
    "            # backprop and update weights\n",
    "            _, loss, total = sess.run([self.train_step, self.loss, self.total])\n",
    "\n",
    "            # store total reward\n",
    "            total = np.mean(total)\n",
    "            totals.append(total)\n",
    "\n",
    "            # store loss information\n",
    "            losses.append(loss)\n",
    "\n",
    "            # show information\n",
    "            if show_progress:\n",
    "                print('Epoch {0:5}: loss = {1}\\r'.format(epoch_idx, loss, total), end='')\n",
    "        print()\n",
    "\n",
    "        return losses, totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_loss_function(graph, rewards):\n",
    "    \n",
    "    with graph.as_default():\n",
    "        total = tf.reduce_sum(rewards, axis=1)\n",
    "        loss  = tf.reduce_mean(tf.square(total))\n",
    "    \n",
    "    return total, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(graph, optimizer, epoch):\n",
    "\n",
    "    # saver\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # optimize it, babe!\n",
    "        losses, totals = optimizer.run(sess, epoch)\n",
    "\n",
    "        end = time.time()\n",
    "        uptime = end - start\n",
    "        print(\"Done in {0:.6f} sec.\\n\".format(uptime))\n",
    "\n",
    "        # save model\n",
    "        save_path = saver.save(sess, 'checkpoints/model.ckpt')\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    return losses, totals, saver, uptime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(graph, series, initial_state, timesteps):\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # restore learned policy model\n",
    "        saver.restore(sess, 'checkpoints/model.ckpt')\n",
    "\n",
    "        # simulate MDP trajectories\n",
    "        result = sess.run(series, feed_dict={'inputs:0': timesteps, 'initial_state:0': initial_state})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RNN inputs for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_initial_state(x0, y0, batch_size):\n",
    "    x_init = np.full([batch_size], x0, np.float32)\n",
    "    y_init = np.full([batch_size], y0, np.float32)\n",
    "    position_init   = np.stack([x_init, y_init], axis=1)\n",
    "    likelihood_init = np.zeros_like(position_init)\n",
    "    initial_state   = np.concatenate([position_init, likelihood_init], axis=1)\n",
    "    return initial_state\n",
    "\n",
    "def build_timesteps(batch_size, max_time):\n",
    "    timesteps = [np.arange(start=1.0, stop=max_time + 1.0, dtype=np.float32)]\n",
    "    timesteps = np.repeat(timesteps, batch_size, axis=0)\n",
    "    timesteps = np.reshape(timesteps, (batch_size, max_time, 1))\n",
    "    return timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train our model! Let's first of all create the computational graph to which all necessary operations will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain parameters\n",
    "grid = {\n",
    "    'ndim': 2,\n",
    "    'size': (10.0, 10.0),\n",
    "    'start': (1.0,  5.0),\n",
    "    'goal':  (8.0,  5.0)\n",
    "}\n",
    "\n",
    "deceleration = {\n",
    "    'center': (5.0, 5.0),\n",
    "    'decay': 2.0\n",
    "}\n",
    "\n",
    "# hyperparameters\n",
    "epoch = 200\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "max_time = 9\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "# MDP model\n",
    "mdp = Navigation(graph, grid, deceleration)\n",
    "\n",
    "# define policy network\n",
    "layers = [mdp.state_size + 1, 20, 5, mdp.action_size]\n",
    "policy = PolicyNetwork(graph, layers)\n",
    "\n",
    "# RNN inputs\n",
    "# timesteps\n",
    "timesteps = build_timesteps(batch_size, max_time)\n",
    "\n",
    "# initial state\n",
    "x0, y0 = grid['start']\n",
    "initial_state = build_initial_state(x0, y0, batch_size)\n",
    "\n",
    "# unroll MDP model\n",
    "rnn = MDP_RNN(mdp, policy, batch_size)\n",
    "rewards, states, actions, final_state = rnn.unroll(initial_state, timesteps)\n",
    "\n",
    "# loss function\n",
    "total, loss = MSE_loss_function(graph, rewards)\n",
    "\n",
    "# optimizer\n",
    "optimizer = PolicyOptimizer(graph, loss, total, learning_rate)\n",
    "\n",
    "# let's train the model!\n",
    "losses, totals, saver, uptime = train(graph, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate the learned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulations\n",
    "simulation_batch_size = 10000\n",
    "simulation_max_time = 9\n",
    "s0 = build_initial_state(x0, y0, simulation_batch_size)\n",
    "timesteps = build_timesteps(simulation_batch_size, simulation_max_time)\n",
    "_, _, _, total_cost = simulate(graph, [rewards, states, actions, total], s0, timesteps)\n",
    "total_cost = np.mean(total_cost)\n",
    "print('>> Average Total Cost = {}'.format(total_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss function and cost per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# plotting losses\n",
    "ax = fig.add_subplot(121)\n",
    "utils.plot_loss_function(ax, losses, epoch)\n",
    "\n",
    "# histogram of cumulative cost per batch\n",
    "ax = fig.add_subplot(122)\n",
    "utils.plot_average_total_cost(ax, totals, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the learned policy in an action grid for different timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_initial_states_grid(x_grid_size, y_grid_size, timestep):\n",
    "    batch_size = x_grid_size * y_grid_size\n",
    "    x_grid = np.linspace(0.0, grid['size'][0], x_grid_size)\n",
    "    y_grid = np.linspace(0.0, grid['size'][1], y_grid_size)\n",
    "    initial_states_grid = []\n",
    "    for x in x_grid:\n",
    "        for y in y_grid:\n",
    "            initial_states_grid.append([x, y, timestep])\n",
    "    return initial_states_grid, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(grid_size, state_size, timesteps):\n",
    "    initial_states_grids = []\n",
    "    actions = []\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        \n",
    "        # re-initialize initial_state placeholder\n",
    "        batch_size = grid_size * grid_size\n",
    "        initial_state = tf.placeholder(shape=(batch_size, state_size), dtype=np.float32, name='initial_state')\n",
    "\n",
    "        # re-initialize policy\n",
    "        with tf.variable_scope('rnn'):\n",
    "            policy = PolicyNetwork(graph, layers)\n",
    "            action = policy(initial_state)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            saver.restore(sess, 'checkpoints/model.ckpt')\n",
    "            \n",
    "            for timestep in timesteps:\n",
    "\n",
    "                # instantiate initial states in a grid \n",
    "                s0, batch_size = build_initial_states_grid(grid_size, grid_size, timestep)\n",
    "                initial_states_grids.append(s0)\n",
    "\n",
    "                # evaluate policy for given initial states\n",
    "                a = sess.run(action, feed_dict={initial_state: s0})\n",
    "                actions.append(a)\n",
    "    \n",
    "    return initial_states_grids, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "timesteps = np.array([0.0, max_time / 3, 2 / 3 * max_time, max_time], dtype=np.int32)\n",
    "initial_states_grid, policy_actions = evaluate_policy(grid_size, mdp.state_size + 1, timesteps)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "for i, timestep in enumerate(timesteps):\n",
    "    ax = fig.add_subplot(1, len(timesteps), i+1)\n",
    "    utils.plot_policy(ax, grid, deceleration, initial_states_grid[i], policy_actions[i], timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate policy for different start states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "\n",
    "# initial states for simulation\n",
    "x0, y0 = grid['start']\n",
    "delta_y = [0.0, -1.0, -1.5, 1.5, 1.0]\n",
    "initial_states = []\n",
    "for delta in delta_y:\n",
    "    s0 = build_initial_state(x0, y0 + delta, batch_size)\n",
    "    initial_states.append(s0)\n",
    "initial_states = np.concatenate(initial_states, axis=0)\n",
    "\n",
    "# total batch_size\n",
    "batch_size = initial_states.shape[0]\n",
    "\n",
    "# timestep\n",
    "timesteps = build_timesteps(batch_size, max_time)\n",
    "\n",
    "# simulate!\n",
    "rewards, states, actions = simulate(graph, [rewards, states, actions], initial_states, timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot simulated trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 25))\n",
    "utils.plot_simulations(fig, grid, deceleration, initial_states, delta_y, states, actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
