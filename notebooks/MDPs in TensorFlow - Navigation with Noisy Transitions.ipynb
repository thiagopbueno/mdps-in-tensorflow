{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling MDPs in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes defining MDPs must inherit from abstract class ```MDP```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class MDP(metaclass=abc.ABCMeta):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def transition(self, state, action):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def reward(self, state, action):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation in 2D grid with deceleration zone at the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Navigation(MDP):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.state_size = self.ndim\n",
    "        self.action_size = self.ndim\n",
    "\n",
    "        # grid constants\n",
    "        self.__size = tf.constant(self.size, dtype=tf.float32)\n",
    "        self.__center = tf.constant(self.center, dtype=tf.float32)\n",
    "        self.__goal = tf.constant(self.goal, dtype=tf.float32)\n",
    "\n",
    "        # numerical constants\n",
    "        self.__0_00 = tf.constant(0.00, dtype=tf.float32)\n",
    "        self.__0_99 = tf.constant(0.99, dtype=tf.float32)\n",
    "        self.__1_00 = tf.constant(1.00, dtype=tf.float32)\n",
    "        self.__2_00 = tf.constant(2.00, dtype=tf.float32)\n",
    "        self.__decay = tf.constant(self.decay, dtype=tf.float32)\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        # distance to center of grid\n",
    "        d = tf.sqrt(tf.reduce_sum(tf.square(state - self.__center), 1, keep_dims=True))\n",
    "\n",
    "        # deceleration_factor\n",
    "        deceleration = self.__2_00 / (self.__1_00 + tf.exp(-self.__decay * d)) - self.__1_00\n",
    "#         deceleration = self.__1_00\n",
    "        \n",
    "        # next position\n",
    "        next_state = state + deceleration * action\n",
    "        next_state = tf.clip_by_value(next_state, self.__0_00, self.__size)\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reward(self, state, action):\n",
    "        return -tf.reduce_sum(tf.abs(state - self.__goal), 1, keep_dims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding an MDP as a Recurrent Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy(state, limits=1.0, n_h=1000):\n",
    "    W1 = tf.get_variable(\"W1\", shape=[n_h, state.shape[1]],\n",
    "                         dtype=tf.float32, initializer=tf.glorot_normal_initializer())\n",
    "    b1 = tf.get_variable(\"b1\", shape=[n_h, 1],\n",
    "                         dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "    W2 = tf.get_variable(\"W2\", shape=[2, n_h],\n",
    "                         dtype=tf.float32, initializer=tf.glorot_normal_initializer())\n",
    "    b2 = tf.get_variable(\"b2\", shape=[2, 1],\n",
    "                         dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    A = tf.nn.relu(tf.matmul(W1, tf.transpose(state)) + b1)\n",
    "    action = tf.transpose(tf.constant(limits) * tf.nn.tanh(tf.matmul(W2, A) + b2))\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDP_RNNCell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, mdp, policy):\n",
    "        self.mdp = mdp\n",
    "        self.policy = policy\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return mdp.state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return mdp.state_size + 2*mdp.action_size + 1 + 2\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        # choose action from policy\n",
    "        action = self.policy(state)\n",
    "\n",
    "        # apply rotation noise\n",
    "        theta = 15 # degrees\n",
    "        cos, sin = tf.cos(theta * np.pi / 180 * inputs), tf.sin(theta * np.pi / 180 * inputs)\n",
    "        \n",
    "        noise = tf.stack([ cos, -sin, sin, cos], axis=1)\n",
    "        noise = tf.reshape(noise, [-1, 2, 2])\n",
    "        noisy_action = tf.matmul(noise, tf.reshape(action, [-1, 2, 1]))\n",
    "        noisy_action = tf.reshape(noisy_action, [-1, 2])\n",
    "        \n",
    "        cos_sin = tf.reshape(tf.stack([cos, sin], axis=1), [-1, 2])\n",
    "\n",
    "        # add MDP components to the RNN cell output\n",
    "        next_state =  self.mdp.transition(state, noisy_action)\n",
    "        reward = self.mdp.reward(next_state, noisy_action)\n",
    "\n",
    "        return tf.concat([reward, next_state, action, noisy_action, cos_sin], 1), next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDP_RNN(object):\n",
    "    \n",
    "    def __init__(self, mdp, policy, batch_size=1):\n",
    "        self.cell = MDP_RNNCell(mdp, policy)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def unroll(self, inputs, initial_state=None):\n",
    "        # set initial state \n",
    "        if initial_state is None:\n",
    "            initial_state = self.cell.zero_state(self.batch_size, dtype=tf.float32)           \n",
    "\n",
    "        # dynamic time unrolling\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(\n",
    "            self.cell,\n",
    "            inputs,\n",
    "            initial_state=initial_state,\n",
    "            dtype=tf.float32)\n",
    "\n",
    "        # gather reward, state and action series\n",
    "        outputs = tf.unstack(outputs, axis=2)\n",
    "        max_time = int(inputs.shape[1])\n",
    "\n",
    "        reward_series = tf.reshape(outputs[0], [-1, max_time, 1])\n",
    "        state_series = tf.stack(outputs[1:3], axis=2)\n",
    "        action_series = tf.stack(outputs[3:5], axis=2)\n",
    "        noisy_action_series = tf.stack(outputs[5:7], axis=2)\n",
    "        \n",
    "        cos_sin = tf.stack(outputs[7:9], axis=2)\n",
    "\n",
    "        return reward_series, state_series, action_series, noisy_action_series, final_state, cos_sin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the action optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyOptimizer(object):\n",
    "    \n",
    "    def __init__(self, loss, learning_rate, size):\n",
    "        self.loss = loss\n",
    "        \n",
    "        # input size\n",
    "        self.size = size\n",
    "\n",
    "        # optimization hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # backprop via RMSProp\n",
    "        self.train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    def run(self, sess, epoch=100, show_progress=True):\n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "#         variables = sess.run({ var.name: var for var in tf.trainable_variables() })\n",
    "#         print(\"Before training ...\")\n",
    "#         for name, var in variables.items():\n",
    "#             print(name)\n",
    "#             print(var)\n",
    "#             print()\n",
    "        \n",
    "        losses = []\n",
    "        for epoch_idx in range(epoch):\n",
    "            # sample noise data\n",
    "            inputs_data = np.random.normal(size=self.size).astype(np.float32)\n",
    "\n",
    "            # backprop and update weights\n",
    "            _, loss = sess.run([self.train_step, self.loss], feed_dict={inputs: inputs_data})\n",
    "\n",
    "            # store and show loss information\n",
    "            losses.append(loss)\n",
    "            if show_progress:\n",
    "                print('Epoch {0:5}: loss = {1}\\r'.format(epoch_idx, loss), end='')\n",
    "        print()\n",
    "    \n",
    "        variables = sess.run({ var.name: var for var in tf.trainable_variables() })\n",
    "#         print(\"After training ...\")\n",
    "#         for name, var in variables.items():\n",
    "#             print(name)\n",
    "#             print(var)\n",
    "#             print()\n",
    "\n",
    "        return losses, variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the MDP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'ndim': 2,\n",
    "    'size': (10.0, 10.0),\n",
    "    'initial': (1.0, 5.0),\n",
    "    'goal': (9.0, 5.0),\n",
    "    'center': (5.0, 5.0),\n",
    "    'decay': 2.0,\n",
    "    'limits': (-1.0, 1.0)\n",
    "}\n",
    "\n",
    "# MDP model\n",
    "mdp = Navigation(**params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(batch_size=1000, max_time=10, epoch=200, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        losses, variables = PolicyOptimizer(loss, learning_rate, (batch_size, max_time, 1)).run(sess, epoch)\n",
    "    end = time.time()\n",
    "    uptime = end - start\n",
    "    print(\"Done in {0:.6f} sec\".format(uptime))\n",
    "    return losses, variables, uptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid_size = y_grid_size = 100\n",
    "\n",
    "max_time = 9\n",
    "batch_size = x_grid_size * y_grid_size\n",
    "epoch=100\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# inputs\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, max_time, 1], name=\"inputs\")\n",
    "\n",
    "# initial state\n",
    "x_grid = np.linspace(0.0, params['size'][0], x_grid_size)\n",
    "y_grid = np.linspace(0.0, params['size'][1], y_grid_size)\n",
    "initial_states_grid = []\n",
    "for x in x_grid:\n",
    "    for y in y_grid:\n",
    "        initial_states_grid.append([x, y])\n",
    "initial_state = tf.constant(initial_states_grid)\n",
    "\n",
    "# unrolled MDP model\n",
    "rnn = MDP_RNN(mdp, policy, batch_size)\n",
    "rewards, states, actions, _, final_state, _ = rnn.unroll(inputs, initial_state)\n",
    "\n",
    "# loss based on total reward\n",
    "total = tf.reduce_sum(rewards, 1)\n",
    "loss = tf.reduce_mean(tf.square(total))\n",
    "\n",
    "# optimize it, babe!\n",
    "losses, variables, uptime = train(batch_size, max_time, epoch, learning_rate)\n",
    "# for name, var in variables.items():\n",
    "#     print(name)\n",
    "#     print(var)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate(series, batch_size=1, max_time=10):\n",
    "    with tf.Session() as sess:\n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # sample noise data\n",
    "        inputs_data = np.random.normal(size=(batch_size, max_time, 1)).astype(np.float32)\n",
    "\n",
    "        # run MDP series\n",
    "        result = sess.run(series, feed_dict={inputs: inputs_data})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 9\n",
    "batch_size = 3\n",
    "\n",
    "# initial state\n",
    "# initial_state = tf.constant([params['initial']], dtype=tf.float32)\n",
    "x_start, y_start = params['initial']\n",
    "x_initial = tf.fill([batch_size], tf.constant(x_start, tf.float32))\n",
    "y_initial = tf.fill([batch_size], tf.constant(y_start, tf.float32))\n",
    "initial_state = tf.stack([x_initial, y_initial], axis=1)\n",
    "delta = [-3.0, -1.5, 1.5, 3.0]\n",
    "for delta_y in delta:\n",
    "    x_initial = tf.fill([batch_size], tf.constant(x_start, tf.float32))\n",
    "    y_initial = tf.fill([batch_size], tf.constant(y_start + delta_y, tf.float32))\n",
    "    initial_state = tf.concat([initial_state, tf.stack([x_initial, y_initial], axis=1)], axis=0)\n",
    "\n",
    "batch_size = initial_state.shape[0]\n",
    "\n",
    "# inputs\n",
    "inputs = tf.placeholder(tf.float32, shape=[batch_size, max_time, 1], name=\"inputs\")\n",
    "\n",
    "def trained_policy(state, init, limits=1.0, n_h=1000):\n",
    "    W1_new = tf.get_variable(\"W1_new\", shape=[n_h, state.shape[1]],\n",
    "                             dtype=tf.float32, initializer=tf.constant_initializer(init[\"rnn/W1:0\"]))\n",
    "    b1_new = tf.get_variable(\"b1_new\", shape=[n_h, 1],\n",
    "                             dtype=tf.float32, initializer=tf.constant_initializer(init[\"rnn/b1:0\"]))\n",
    "    W2_new = tf.get_variable(\"W2_new\", shape=[2, n_h],\n",
    "                             dtype=tf.float32, initializer=tf.constant_initializer(init[\"rnn/W2:0\"]))\n",
    "    b2_new = tf.get_variable(\"b2_new\", shape=[2, 1],\n",
    "                             dtype=tf.float32, initializer=tf.constant_initializer(init[\"rnn/b2:0\"]))\n",
    "    \n",
    "    A = tf.nn.relu(tf.matmul(W1_new, tf.transpose(state)) + b1_new)\n",
    "    action = tf.transpose(tf.constant(limits) * tf.nn.tanh(tf.matmul(W2_new, A) + b2_new))\n",
    "\n",
    "    return action\n",
    "\n",
    "# unrolled MDP model\n",
    "policy = lambda s: trained_policy(s, variables)\n",
    "rnn = MDP_RNN(mdp, policy, batch_size)\n",
    "rewards, states, actions, noisy_action, final_state, cos_sin = rnn.unroll(inputs, initial_state)\n",
    "\n",
    "# simulate!\n",
    "r_series, s_series, a_series, n_series, cos_sin = simulate([rewards, states, actions, noisy_action, cos_sin], batch_size, max_time)\n",
    "# print(s_series)\n",
    "# print()\n",
    "# print(a_series)\n",
    "# r_series, s_series, a_series, n_series, cos_sin = np.squeeze(r_series), np.squeeze(s_series), np.squeeze(a_series), np.squeeze(n_series), np.squeeze(cos_sin)\n",
    "# print(\"Action, Noise, Noisy Action, State, Reward\")\n",
    "# i = 1\n",
    "# for a, n, s, r, x in zip(a_series, n_series, s_series, r_series, cos_sin):\n",
    "#     print(i, end=', ')\n",
    "#     print(\"[{0:-10.6f}, {1:-10.6f}]\".format(a[0], a[1]), end=', ')\n",
    "#     print(\"[{0:-10.6f}, {1:-10.6f}]\".format(x[0], x[1]), end=', ')\n",
    "#     print(\"[{0:-10.6f}, {1:-10.6f}]\".format(n[0], n[1]), end=', ')\n",
    "#     print(\"[{0:-10.6f}, {1:-10.6f}]\".format(s[0], s[1]), end=', ')\n",
    "#     print(\"{0:-10.6f}\".format(r))\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# plotting losses\n",
    "plt.subplot(121)\n",
    "plt.plot(losses, 'b-')\n",
    "plt.xlim(0, epoch)\n",
    "plt.title('Total Loss')\n",
    "plt.xlabel(\"# iterations\")\n",
    "plt.ylabel(\"total loss\")\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_navigation(ax, start, end, s_series, a_series, deceleration=True, initial=False):\n",
    "    # params\n",
    "    xlim, ylim = params['size']\n",
    "    xcenter, ycenter = params['center']\n",
    "    \n",
    "    # plot configuration\n",
    "    ax.axis([0.0, xlim, 0.0, ylim])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid()\n",
    "#     ax.title(\"Navigation trajectory and deceleration zone\", fontweight=\"bold\", fontsize=16)\n",
    "    ax.set_xlabel(\"x coordinate\")\n",
    "    ax.set_ylabel(\"y coordinate\")\n",
    "#     ax.legend(loc='lower right')\n",
    "\n",
    "    if deceleration:\n",
    "        npoints = 1000\n",
    "        X, Y = np.meshgrid(np.linspace(0.0, xlim, npoints), np.linspace(0.0, ylim, npoints))\n",
    "        D = np.sqrt((X - xcenter) ** 2 + (Y - ycenter) ** 2)\n",
    "        Lambda = 2 / (1 + np.exp(-params['decay'] * D)) - 1.00\n",
    "\n",
    "        ticks = np.arange(0.0, 1.01, 0.10)\n",
    "        cp = ax.contourf(X, Y, Lambda, ticks, cmap=plt.cm.bone)\n",
    "        # ax.colorbar(cp, ticks=ticks)\n",
    "        cp = ax.contour(X, Y, Lambda, ticks, colors='black', linestyles='dashed')\n",
    "        # ax.clabel(cp, inline=True, fontsize=10)\n",
    "\n",
    "    if initial:\n",
    "        initial_states_x = [ p[0] for p in initial_states_grid ]\n",
    "        initial_states_y = [ p[1] for p in initial_states_grid ]\n",
    "        ax.plot(initial_states_x, initial_states_y, 'gx')\n",
    "\n",
    "    # actions\n",
    "    positions = np.concatenate([[start], s_series])\n",
    "    ax.quiver(positions[:-1, 0], positions[:-1, 1], a_series[:, 0], a_series[:, 1],\n",
    "              angles='xy', scale_units='xy', scale=1, color='dodgerblue', width=0.005,\n",
    "              label='actions')\n",
    "\n",
    "    # states\n",
    "    ax.plot(positions[:, 0], positions[:, 1], '-', marker='o', color='darkblue', markersize=8, label='states')\n",
    "\n",
    "    # start and end\n",
    "    ax.plot([start[0]], [start[1]], marker='X', markersize=15, color='limegreen', label='initial')\n",
    "    ax.plot([end[0]], [end[1]], marker='X', markersize=15, color='crimson', label='goal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = params['initial'], params['goal']\n",
    "fig = plt.figure(figsize=(15, 25))\n",
    "num_plots = int(initial_state.shape[0])\n",
    "deltas = [0] + delta\n",
    "rows = len(deltas)\n",
    "cols = num_plots // len(deltas)\n",
    "for i in range(num_plots):\n",
    "    ax = fig.add_subplot(len(deltas),num_plots/len(deltas),i+1)\n",
    "    idx = i//cols\n",
    "    start = (params['initial'][0], params['initial'][1] + deltas[idx])\n",
    "    plot_navigation(ax, start, end, s_series[i], a_series[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
